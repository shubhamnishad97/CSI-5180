{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Question Answering on COVID-roberta",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0bba4d5b28d245f8b34f8eec6a23ee7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c5369c1bae2347fbb44076df5c07b4d4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fb0bac7c8c0e4020b4ea241da4dc26b2",
              "IPY_MODEL_f6974ff7c62c467fb3b92d1b532541b4"
            ]
          }
        },
        "c5369c1bae2347fbb44076df5c07b4d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fb0bac7c8c0e4020b4ea241da4dc26b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d7d3214209734f41a9e50893b6fa01fa",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1754,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1754,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0845e1d9dc4c4431995ad70c50ae860a"
          }
        },
        "f6974ff7c62c467fb3b92d1b532541b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b171e48bc39a41c8967a40b63056e879",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4.61k/? [00:01&lt;00:00, 3.05kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d79119404b7f4aa698f5b3ebfd56526d"
          }
        },
        "d7d3214209734f41a9e50893b6fa01fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0845e1d9dc4c4431995ad70c50ae860a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b171e48bc39a41c8967a40b63056e879": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d79119404b7f4aa698f5b3ebfd56526d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d8033a5a14ff4a5ebd0e73d22be08ab3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_90556525580c407d991f0e168df7b91d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_405d86eb2d1049a9abfd86b9d18dcff3",
              "IPY_MODEL_55c2b437d0c14ef79823eac786c235f3"
            ]
          }
        },
        "90556525580c407d991f0e168df7b91d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "405d86eb2d1049a9abfd86b9d18dcff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_aa0d67677e9748dfa3ffcc0ca3696fbf",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 794,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 794,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_267e7a794c3f44288484e0f9c9d7fe4f"
          }
        },
        "55c2b437d0c14ef79823eac786c235f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ff2b680367264e1f8f04130795d9f6d6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.80k/? [00:00&lt;00:00, 23.2kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9aa75db88040419a9211660e51c03400"
          }
        },
        "aa0d67677e9748dfa3ffcc0ca3696fbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "267e7a794c3f44288484e0f9c9d7fe4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ff2b680367264e1f8f04130795d9f6d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9aa75db88040419a9211660e51c03400": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "62b8c7d7539e48e68c03dc50af3b1365": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_895c5135f73b426a8995975b99901a5d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ba17fafb29794ed4a310dfe01eecd4e6",
              "IPY_MODEL_fa0e1c4d20664fbd96d18c6baaef5266"
            ]
          }
        },
        "895c5135f73b426a8995975b99901a5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ba17fafb29794ed4a310dfe01eecd4e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_882849db704e4782a3876c6b16bc1541",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1346251,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1346251,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_22963a5182884e58b4769001c115475d"
          }
        },
        "fa0e1c4d20664fbd96d18c6baaef5266": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d0af35613b934327a0839e35696097bd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4.42M/? [00:00&lt;00:00, 24.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5230c807882744249ec9133327be10ba"
          }
        },
        "882849db704e4782a3876c6b16bc1541": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "22963a5182884e58b4769001c115475d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d0af35613b934327a0839e35696097bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5230c807882744249ec9133327be10ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4744ec0f7c394fa0bdf24d13925e96fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f1badd561eaf44b498eb21d5e61305c3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e6f4f8ba88484099b35e0396cb4b8dcf",
              "IPY_MODEL_b240e7f0b7354f1587e025f420644771"
            ]
          }
        },
        "f1badd561eaf44b498eb21d5e61305c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e6f4f8ba88484099b35e0396cb4b8dcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a3e1eae5a77643bd986224811fe396f8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_311612cc61d3473495a908a11ceb1803"
          }
        },
        "b240e7f0b7354f1587e025f420644771": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_90db1dcc5b054b048757e852bfe036f0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2019/0 [00:03&lt;00:00,  2.61s/ examples]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eb1cf19b4932499e98a1b66a32db7829"
          }
        },
        "a3e1eae5a77643bd986224811fe396f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "311612cc61d3473495a908a11ceb1803": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "90db1dcc5b054b048757e852bfe036f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eb1cf19b4932499e98a1b66a32db7829": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "89397a070a364f39b7bebf8217ad6302": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_71d07444f98a4ba88db9aa2b36d06d03",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d4cf26cd549749fc8cdc28625cf734d4",
              "IPY_MODEL_77e1ab9debc1482a8ff75e949f3b2d72"
            ]
          }
        },
        "71d07444f98a4ba88db9aa2b36d06d03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d4cf26cd549749fc8cdc28625cf734d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9e19357ed2bc43feb5e2712a36635fc7",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1265,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1265,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_610f08762e85494889126e893711722d"
          }
        },
        "77e1ab9debc1482a8ff75e949f3b2d72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6f6c16f7573c4d33af74ad764f825d10",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.26k/1.26k [00:00&lt;00:00, 2.71kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1772a435ec624f369b04d32fb5bee060"
          }
        },
        "9e19357ed2bc43feb5e2712a36635fc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "610f08762e85494889126e893711722d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6f6c16f7573c4d33af74ad764f825d10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1772a435ec624f369b04d32fb5bee060": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7b51e48317dd449b9d81293886921935": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9e15846cd4634c34ab7efc63bd9f99a9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_97129f3a6cce4423800237fe7a45e2a7",
              "IPY_MODEL_16a440cee8cc48a091d29ffcfa3e230b"
            ]
          }
        },
        "9e15846cd4634c34ab7efc63bd9f99a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "97129f3a6cce4423800237fe7a45e2a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_62b17418670041e9b8487d9e921d9363",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898822,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898822,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_109093c53bc940cba07227b695e1381f"
          }
        },
        "16a440cee8cc48a091d29ffcfa3e230b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e607cfe24e8e45ebab988b26dd35718d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 899k/899k [00:02&lt;00:00, 359kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_838b5ddb8913428589d3a874e815d44f"
          }
        },
        "62b17418670041e9b8487d9e921d9363": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "109093c53bc940cba07227b695e1381f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e607cfe24e8e45ebab988b26dd35718d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "838b5ddb8913428589d3a874e815d44f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bac3d706bef242adb91c920d1718e374": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d192adc1f27d468e9a6cdade8343d2ef",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f0442257f23047db9177b89325918b97",
              "IPY_MODEL_b3d6707cb3914af2829a2923257ffe6b"
            ]
          }
        },
        "d192adc1f27d468e9a6cdade8343d2ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f0442257f23047db9177b89325918b97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_06cccb63e3c548718e5e35852561c830",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b7f35b0978bf4f2ab3f48d4b1478d05a"
          }
        },
        "b3d6707cb3914af2829a2923257ffe6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0b48ad8149e84916a67f0731c8a9bcc6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:01&lt;00:00, 386kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_50afd1d590b342449903c9d904f20c4a"
          }
        },
        "06cccb63e3c548718e5e35852561c830": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b7f35b0978bf4f2ab3f48d4b1478d05a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0b48ad8149e84916a67f0731c8a9bcc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "50afd1d590b342449903c9d904f20c4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cee17275fa7045f5aa931e6d30c2ed53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7ae2880bd5f7429287dfadeb39a80562",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_38165347fc38458e8a75e89e5df68682",
              "IPY_MODEL_f35749c87d29483789417fd06967d515"
            ]
          }
        },
        "7ae2880bd5f7429287dfadeb39a80562": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "38165347fc38458e8a75e89e5df68682": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_83820ce4a5f7403eb5439ae0b455366d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 150,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 150,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5be5de12f3a347aa8e9920b0d6402af3"
          }
        },
        "f35749c87d29483789417fd06967d515": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9c979b3347734e7db714ed235c443338",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 150/150 [00:00&lt;00:00, 272B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_53bfb1f33ab94c8782987dad587074ee"
          }
        },
        "83820ce4a5f7403eb5439ae0b455366d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5be5de12f3a347aa8e9920b0d6402af3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9c979b3347734e7db714ed235c443338": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "53bfb1f33ab94c8782987dad587074ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c3c193688f12460ea978db4dbc909ebb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_355d49d957ae4035850578dfc1b05093",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ac7fad6dd4774ab08656bca8733b04ee",
              "IPY_MODEL_e70d425c11af45baa9fff194eb9b8520"
            ]
          }
        },
        "355d49d957ae4035850578dfc1b05093": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ac7fad6dd4774ab08656bca8733b04ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_10a2f525393744e6bd2576d56c25889f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 190,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 190,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ce96794a5a6248d3875945e7779b0eab"
          }
        },
        "e70d425c11af45baa9fff194eb9b8520": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_04e14eb14ae84ce19121426e7ee9c932",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 190/190 [00:00&lt;00:00, 2.02kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_16b351bf68bb43eda4dc6ff41ab06dc7"
          }
        },
        "10a2f525393744e6bd2576d56c25889f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ce96794a5a6248d3875945e7779b0eab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "04e14eb14ae84ce19121426e7ee9c932": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "16b351bf68bb43eda4dc6ff41ab06dc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e181d12d7990447f9247fb42c5690359": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_98fa55a47b0742d3925f6ee99fd1a1dd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fea4c112ef464e7f84eda0429b190e36",
              "IPY_MODEL_c9f938f970ea4736a3e6d7d00d4c12ef"
            ]
          }
        },
        "98fa55a47b0742d3925f6ee99fd1a1dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fea4c112ef464e7f84eda0429b190e36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ff9e01352f644059b716c0ecab7bcac5",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c5e392adda61409a992f2bb26c6f9703"
          }
        },
        "c9f938f970ea4736a3e6d7d00d4c12ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b9c55dc3f966404ea2135b91c9c1bb99",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/2 [00:38&lt;00:00, 19.28s/ba]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_44cfb14a0323448aa773cfeb006be369"
          }
        },
        "ff9e01352f644059b716c0ecab7bcac5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c5e392adda61409a992f2bb26c6f9703": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b9c55dc3f966404ea2135b91c9c1bb99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "44cfb14a0323448aa773cfeb006be369": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "648c7cba7b9246c2adc76dfacc56f035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f5707967437d4073a400cf3264573bf6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e05e419c88ee4364978d9ca52b1e6801",
              "IPY_MODEL_5b18a06e2b294c2094f80a2d00e2270d"
            ]
          }
        },
        "f5707967437d4073a400cf3264573bf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e05e419c88ee4364978d9ca52b1e6801": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c7a26ef462c64596beee2fbe8d1f2dab",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_590714fc719b42c1a0d214cffce7299c"
          }
        },
        "5b18a06e2b294c2094f80a2d00e2270d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_782c161bada94987ade01845b6377597",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:03&lt;00:00,  3.92s/ba]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_92163e0ed63148f78f5bd6dc333aa479"
          }
        },
        "c7a26ef462c64596beee2fbe8d1f2dab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "590714fc719b42c1a0d214cffce7299c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "782c161bada94987ade01845b6377597": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "92163e0ed63148f78f5bd6dc333aa479": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b671a9cd6cbe48c2bc8cca26f03b61a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_46723feb4f2f43aea7583f0b075e5e4b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d3dad21ac94c4ef7a569dde62221fe0b",
              "IPY_MODEL_9f841c51ba0947b298d2a897e502b36e"
            ]
          }
        },
        "46723feb4f2f43aea7583f0b075e5e4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d3dad21ac94c4ef7a569dde62221fe0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6c91a79738e9416a8bac9c370fe71343",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 498637768,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 498637768,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5e792c123efd42be8388ecf72e484c8f"
          }
        },
        "9f841c51ba0947b298d2a897e502b36e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_99c538b16a444d25a329b2ad03f9062e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 499M/499M [00:12&lt;00:00, 39.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_78eb26f05c124087ba4f989fe63680f1"
          }
        },
        "6c91a79738e9416a8bac9c370fe71343": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5e792c123efd42be8388ecf72e484c8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "99c538b16a444d25a329b2ad03f9062e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "78eb26f05c124087ba4f989fe63680f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1665f22fa1754e169082e4cdfd73fc09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_931b8585ad2748fb8ade27e0e26c2d67",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_de90c21651f5453b862739456d81b96d",
              "IPY_MODEL_6f0c56daa66c43c998fc85ef671299d5"
            ]
          }
        },
        "931b8585ad2748fb8ade27e0e26c2d67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "de90c21651f5453b862739456d81b96d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e372e6baab444b2d8a5d0b1acebe6daa",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dd4656abe3514bc2b4d932c06ed741c7"
          }
        },
        "6f0c56daa66c43c998fc85ef671299d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ec1981c4de6e4fdbb2f39997ce30b5b5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:14&lt;00:00, 14.20s/ba]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6b6c495342d347dc8b4cc23666623eb7"
          }
        },
        "e372e6baab444b2d8a5d0b1acebe6daa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dd4656abe3514bc2b4d932c06ed741c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ec1981c4de6e4fdbb2f39997ce30b5b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6b6c495342d347dc8b4cc23666623eb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e97bf3ca5e084f70b58ffe6713ec50be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_459dba16a7dc4e28a383aa3b89f97092",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ae4399e23f734edf8ffa9ceb32c888f8",
              "IPY_MODEL_87b5d966b952402ba788e8cf678c62c5"
            ]
          }
        },
        "459dba16a7dc4e28a383aa3b89f97092": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ae4399e23f734edf8ffa9ceb32c888f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fd4c8cd51d83460f8c7dfd43a78db358",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 300,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 300,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e281bc087217403ebb5fc2d2d9f62eca"
          }
        },
        "87b5d966b952402ba788e8cf678c62c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e7512d90ba7b4795bd974405254c48ec",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 300/300 [00:13&lt;00:00, 22.59it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9a426f65cc3c4b23887b165a75bf4d38"
          }
        },
        "fd4c8cd51d83460f8c7dfd43a78db358": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e281bc087217403ebb5fc2d2d9f62eca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e7512d90ba7b4795bd974405254c48ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9a426f65cc3c4b23887b165a75bf4d38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4cRE8IbIrIV"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install 🤗 Transformers and 🤗 Datasets. Uncomment the following cell and run it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If9gpSrtt1bT",
        "outputId": "255954ac-4ef0-4ba1-e408-cc7c650e27cb"
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n",
        "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp37-none-any.whl size=7411 sha256=40242dfc8554716ee205231f0c7bf3611e4a7c34e743628efca117396d78e279\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.8 GB  |     Proc size: 118.3 MB\n",
            "GPU RAM Free: 15109MB | Used: 0MB | Util   0% | Total     15109MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZChVm6f-CIR",
        "outputId": "7b2e49b2-a971-44c4-d69c-3dd761c68c67"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "source": [
        "! pip install datasets transformers > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFASsisvIrIb"
      },
      "source": [
        "If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries.\n",
        "\n",
        "You can find a script version of this notebook to fine-tune your model in a distributed fashion using multiple GPUs or TPUs [here](https://github.com/huggingface/transformers/tree/master/examples/question-answering)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "# Fine-tuning a model on a question-answering task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chPBzdBU9csw"
      },
      "source": [
        "In this notebook, we will see how to fine-tune one of the [🤗 Transformers](https://github.com/huggingface/transformers) model to a question answering task, which is the task of extracting the answer to a question from a given context. We will see how to easily load a dataset for these kinds of tasks and use the `Trainer` API to fine-tune a model on it.\n",
        "\n",
        "![Widget inference representing the QA task](https://github.com/huggingface/notebooks/blob/master/examples/images/question_answering.png?raw=1)\n",
        "\n",
        "**Note:** This notebook finetunes models that answer question by taking a substring of a context, not by generating new text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RRkXuteIrIh"
      },
      "source": [
        "This notebook is built to run on any question answering task with the same format as SQUAD (version 1 or 2), with any model checkpoint from the [Model Hub](https://huggingface.co/models) as long as that model has a version with a token classification head and a fast tokenizer (check on [this table](https://huggingface.co/transformers/index.html#bigtable) if this is the case). It might just need some small adjustments if you decide to use a different dataset than the one used here. Depending on you model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set those three parameters, then the rest of the notebook should run smoothly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVvslsfMIrIh"
      },
      "source": [
        "# This flag is the difference between SQUAD v1 or 2 (if you're using another dataset, it indicates if impossible\n",
        "# answers are allowed or not).\n",
        "squad_v2 = False\n",
        "model_checkpoint = \"deepset/roberta-base-squad2-covid\"\n",
        "batch_size = 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7QYTpxXIrIl"
      },
      "source": [
        "We will use the [🤗 Datasets](https://github.com/huggingface/datasets) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions `load_dataset` and `load_metric`.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IreSlFmlIrIm"
      },
      "source": [
        "from datasets import load_dataset, load_metric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKx2zKs5IrIq"
      },
      "source": [
        "For our example here, we'll use the [SQUAD dataset](https://rajpurkar.github.io/SQuAD-explorer/). The notebook should work with any question answering dataset provided by the 🤗 Datasets library. If you're using your own dataset defined from a JSON or csv file (see the [Datasets documentation](https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files) on how to load them), it might need some adjustments in the names of the columns used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218,
          "referenced_widgets": [
            "0bba4d5b28d245f8b34f8eec6a23ee7c",
            "c5369c1bae2347fbb44076df5c07b4d4",
            "fb0bac7c8c0e4020b4ea241da4dc26b2",
            "f6974ff7c62c467fb3b92d1b532541b4",
            "d7d3214209734f41a9e50893b6fa01fa",
            "0845e1d9dc4c4431995ad70c50ae860a",
            "b171e48bc39a41c8967a40b63056e879",
            "d79119404b7f4aa698f5b3ebfd56526d",
            "d8033a5a14ff4a5ebd0e73d22be08ab3",
            "90556525580c407d991f0e168df7b91d",
            "405d86eb2d1049a9abfd86b9d18dcff3",
            "55c2b437d0c14ef79823eac786c235f3",
            "aa0d67677e9748dfa3ffcc0ca3696fbf",
            "267e7a794c3f44288484e0f9c9d7fe4f",
            "ff2b680367264e1f8f04130795d9f6d6",
            "9aa75db88040419a9211660e51c03400",
            "62b8c7d7539e48e68c03dc50af3b1365",
            "895c5135f73b426a8995975b99901a5d",
            "ba17fafb29794ed4a310dfe01eecd4e6",
            "fa0e1c4d20664fbd96d18c6baaef5266",
            "882849db704e4782a3876c6b16bc1541",
            "22963a5182884e58b4769001c115475d",
            "d0af35613b934327a0839e35696097bd",
            "5230c807882744249ec9133327be10ba",
            "4744ec0f7c394fa0bdf24d13925e96fe",
            "f1badd561eaf44b498eb21d5e61305c3",
            "e6f4f8ba88484099b35e0396cb4b8dcf",
            "b240e7f0b7354f1587e025f420644771",
            "a3e1eae5a77643bd986224811fe396f8",
            "311612cc61d3473495a908a11ceb1803",
            "90db1dcc5b054b048757e852bfe036f0",
            "eb1cf19b4932499e98a1b66a32db7829"
          ]
        },
        "id": "s_AY1ATSIrIq",
        "outputId": "68053ef3-1c34-4238-883c-d16d2cf8124f"
      },
      "source": [
        "datasets = load_dataset(\"covid_qa_deepset\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0bba4d5b28d245f8b34f8eec6a23ee7c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1754.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8033a5a14ff4a5ebd0e73d22be08ab3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=794.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading and preparing dataset covid_qa_deepset/covid_qa_deepset (download: 4.21 MiB, generated: 62.13 MiB, post-processed: Unknown size, total: 66.35 MiB) to /root/.cache/huggingface/datasets/covid_qa_deepset/covid_qa_deepset/1.0.0/db8b3251603d2c1afd9b1dd8a46d7ab63bce6e3d14d8fc48062fd68b5a0bc6d7...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62b8c7d7539e48e68c03dc50af3b1365",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1346251.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4744ec0f7c394fa0bdf24d13925e96fe",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\rDataset covid_qa_deepset downloaded and prepared to /root/.cache/huggingface/datasets/covid_qa_deepset/covid_qa_deepset/1.0.0/db8b3251603d2c1afd9b1dd8a46d7ab63bce6e3d14d8fc48062fd68b5a0bc6d7. Subsequent calls will reuse this data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg08rbRGFGxc",
        "outputId": "f71c5578-8323-488e-f007-a0b0dfc45493"
      },
      "source": [
        "datasets[\"train\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['document_id', 'context', 'question', 'is_impossible', 'id', 'answers'],\n",
              "    num_rows: 2019\n",
              "})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzfPtOMoIrIu"
      },
      "source": [
        "The `datasets` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6HrpprwIrIz"
      },
      "source": [
        "\n",
        "datasets[\"validation\"] = datasets[\"train\"]\n",
        "datasets[\"validation\"] = datasets[\"validation\"].select([i for i in range(0,300)])\n",
        "datasets[\"train\"] = datasets[\"train\"].select([i for i in range(300,2019)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWiVUF0jIrIv",
        "outputId": "6f27c6bc-d4d5-4216-b1c9-4dd61c66affe"
      },
      "source": [
        "datasets[\"train\"][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answers': {'answer_start': [2040],\n",
              "  'text': ['homeostasis, differentiation, embryonic development, and organ physiology']},\n",
              " 'context': 'iNR-Drug: Predicting the Interaction of Drugs with Nuclear Receptors in Cellular Networking\\n\\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC3975431/\\n\\nSHA: ee55aea26f816403476a7cb71816b8ecb1110329\\n\\nAuthors: Fan, Yue-Nong; Xiao, Xuan; Min, Jian-Liang; Chou, Kuo-Chen\\nDate: 2014-03-19\\nDOI: 10.3390/ijms15034915\\nLicense: cc-by\\n\\nAbstract: Nuclear receptors (NRs) are closely associated with various major diseases such as cancer, diabetes, inflammatory disease, and osteoporosis. Therefore, NRs have become a frequent target for drug development. During the process of developing drugs against these diseases by targeting NRs, we are often facing a problem: Given a NR and chemical compound, can we identify whether they are really in interaction with each other in a cell? To address this problem, a predictor called “iNR-Drug” was developed. In the predictor, the drug compound concerned was formulated by a 256-D (dimensional) vector derived from its molecular fingerprint, and the NR by a 500-D vector formed by incorporating its sequential evolution information and physicochemical features into the general form of pseudo amino acid composition, and the prediction engine was operated by the SVM (support vector machine) algorithm. Compared with the existing prediction methods in this area, iNR-Drug not only can yield a higher success rate, but is also featured by a user-friendly web-server established at http://www.jci-bioinfo.cn/iNR-Drug/, which is particularly useful for most experimental scientists to obtain their desired data in a timely manner. It is anticipated that the iNR-Drug server may become a useful high throughput tool for both basic research and drug development, and that the current approach may be easily extended to study the interactions of drug with other targets as well.\\n\\nText: With the ability to directly bind to DNA ( Figure 1 ) and regulate the expression of adjacent genes, nuclear receptors (NRs) are a class of ligand-inducible transcription factors. They regulate various biological processes, such as homeostasis, differentiation, embryonic development, and organ physiology [1] [2] [3] . The NR superfamily has been classified into seven families: NR0 (knirps or DAX like) [4, 5] ; NR1 (thyroid hormone like), NR2 (HNF4-like), NR3 (estrogen like), NR4 (nerve growth factor IB-like), NR5 (fushi tarazu-F1 like), and NR6 (germ cell nuclear factor like). Since they are involved in almost all aspects of human physiology and are implicated in many major diseases such as cancer, diabetes and osteoporosis, nuclear receptors have become major drug targets [6, 7] , along with G protein-coupled receptors (GPCRs) [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] , ion channels [18] [19] [20] , and kinase proteins [21] [22] [23] [24] . Identification of drug-target interactions is one of the most important steps for the new medicine development [25, 26] . The method usually adopted in this step is molecular docking simulation [27] [28] [29] [30] [31] [32] [33] [34] [35] [36] [37] [38] [39] [40] [41] [42] [43] . However, to make molecular docking study feasible, a reliable 3D (three dimensional) structure of the target protein is the prerequisite condition. Although X-ray crystallography is a powerful tool in determining protein 3D structures, it is time-consuming and expensive. Particularly, not all proteins can be successfully crystallized. For example, membrane proteins are very difficult to crystallize and most of them will not dissolve in normal solvents. Therefore, so far very few membrane protein 3D structures have been determined. Although NMR (Nuclear Magnetic Resonance) is indeed a very powerful tool in determining the 3D structures of membrane proteins as indicated by a series of recent publications (see, e.g., [44] [45] [46] [47] [48] [49] [50] [51] and a review article [20] ), it is also time-consuming and costly. To acquire the 3D structural information in a timely manner, one has to resort to various structural bioinformatics tools (see, e.g., [37] ), particularly the homologous modeling approach as utilized for a series of protein receptors urgently needed during the process of drug development [19, [52] [53] [54] [55] [56] [57] . Unfortunately, the number of dependable templates for developing high quality 3D structures by means of homology modeling is very limited [37] .\\n\\nTo overcome the aforementioned problems, it would be of help to develop a computational method for predicting the interactions of drugs with nuclear receptors in cellular networking based on the sequences information of the latter. The results thus obtained can be used to pre-exclude the compounds identified not in interaction with the nuclear receptors, so as to timely stop wasting time and money on those unpromising compounds [58] .\\n\\nActually, based on the functional groups and biological features, a powerful method was developed recently [59] for this purpose. However, further development in this regard is definitely needed due to the following reasons. (a) He et al. [59] did not provide a publicly accessible web-server for their method, and hence its practical application value is quite limited, particularly for the broad experimental scientists; (b) The prediction quality can be further enhanced by incorporating some key features into the formulation of NR-drug (nuclear receptor and drug) samples via the general form of pseudo amino acid composition [60] .\\n\\nThe present study was initiated with an attempt to develop a new method for predicting the interaction of drugs with nuclear receptors by addressing the two points.\\n\\nAs demonstrated by a series of recent publications [10, 18, [61] [62] [63] [64] [65] [66] [67] [68] [69] [70] and summarized in a comprehensive review [60] , to establish a really effective statistical predictor for a biomedical system, we need to consider the following steps: (a) select or construct a valid benchmark dataset to train and test the predictor; (b) represent the statistical samples with an effective formulation that can truly reflect their intrinsic correlation with the object to be predicted; (c) introduce or develop a powerful algorithm or engine to operate the prediction; (d) properly perform cross-validation tests to objectively evaluate the anticipated accuracy of the predictor; (e) establish a user-friendly web-server for the predictor that is accessible to the public. Below, let us elaborate how to deal with these steps.\\n\\nThe data used in the current study were collected from KEGG (Kyoto Encyclopedia of Genes and Genomes) [71] at http://www.kegg.jp/kegg/. KEGG is a database resource for understanding high-level functions and utilities of the biological system, such as the cell, the organism and the ecosystem, from molecular-level information, especially large-scale molecular datasets generated by genome sequencing and other high-throughput experimental technologies. Here, the benchmark dataset can be formulated as\\n\\nwhere is the positive subset that consists of the interactive drug-NR pairs only, while the negative subset that contains of the non-interactive drug-NR pairs only, and the symbol represents the union in the set theory. The so-called \"interactive\" pair here means the pair whose two counterparts are interacting with each other in the drug-target networks as defined in the KEGG database [71] ; while the \"non-interactive\" pair means that its two counterparts are not interacting with each other in the drug-target networks. The positive dataset contains 86 drug-NR pairs, which were taken from He et al. [59] . The negative dataset contains 172 non-interactive drug-NR pairs, which were derived according to the following procedures: (a) separating each of the pairs in into single drug and NR; (b) re-coupling each of the single drugs with each of the single NRs into pairs in a way that none of them occurred in ; (c) randomly picking the pairs thus formed until reaching the number two times as many as the pairs in . The 86 interactive drug-NR pairs and 172 non-interactive drug-NR pairs are given in Supplementary Information S1, from which we can see that the 86 + 172 = 258 pairs in the current benchmark dataset are actually formed by 25 different NRs and 53 different compounds.\\n\\nSince each of the samples in the current network system contains a drug (compound) and a NR (protein), the following procedures were taken to represent the drug-NR pair sample.\\n\\nFirst, for the drug part in the current benchmark dataset, we can use a 256-D vector to formulate it as given by\\n\\nwhere D represents the vector for a drug compound, and d i its i-th (i = 1,2, ,256) component that can be derived by following the \"2D molecular fingerprint procedure\" as elaborated in [10] . The 53 molecular fingerprint vectors thus obtained for the 53 drugs in are, respectively, given in Supplementary Information S2.\\n\\nThe protein sequences of the 25 different NRs in are listed in Supplementary Information S3. Suppose the sequence of a nuclear receptor protein P with L residues is generally expressed by\\n\\nwhere 1 R represents the 1st residue of the protein sequence P , 2 R the 2nd residue, and so forth. Now the problem is how to effectively represent the sequence of Equation (3) with a non-sequential or discrete model [72] . This is because all the existing operation engines, such as covariance discriminant (CD) [17, 65, [73] [74] [75] [76] [77] [78] [79] , neural network [80] [81] [82] , support vector machine (SVM) [62] [63] [64] 83] , random forest [84, 85] , conditional random field [66] , nearest neighbor (NN) [86, 87] ; K-nearest neighbor (KNN) [88] [89] [90] , OET-KNN [91] [92] [93] [94] , and Fuzzy K-nearest neighbor [10, 12, 18, 69, 95] , can only handle vector but not sequence samples. However, a vector defined in a discrete model may completely lose all the sequence-order information and hence limit the quality of prediction. Facing such a dilemma, can we find an approach to partially incorporate the sequence-order effects? Actually, one of the most challenging problems in computational biology is how to formulate a biological sequence with a discrete model or a vector, yet still keep considerable sequence order information. To avoid completely losing the sequence-order information for proteins, the pseudo amino acid composition [96, 97] or Chou\\'s PseAAC [98] was proposed. Ever since the concept of PseAAC was proposed in 2001 [96] , it has penetrated into almost all the areas of computational proteomics, such as predicting anticancer peptides [99] , predicting protein subcellular location [100] [101] [102] [103] [104] [105] [106] , predicting membrane protein types [107, 108] , predicting protein submitochondria locations [109] [110] [111] [112] , predicting GABA(A) receptor proteins [113] , predicting enzyme subfamily classes [114] , predicting antibacterial peptides [115] , predicting supersecondary structure [116] , predicting bacterial virulent proteins [117] , predicting protein structural class [118] , predicting the cofactors of oxidoreductases [119] , predicting metalloproteinase family [120] , identifying cysteine S-nitrosylation sites in proteins [66] , identifying bacterial secreted proteins [121] , identifying antibacterial peptides [115] , identifying allergenic proteins [122] , identifying protein quaternary structural attributes [123, 124] , identifying risk type of human papillomaviruses [125] , identifying cyclin proteins [126] , identifying GPCRs and their types [15, 16] , discriminating outer membrane proteins [127] , classifying amino acids [128] , detecting remote homologous proteins [129] , among many others (see a long list of papers cited in the References section of [60] ). Moreover, the concept of PseAAC was further extended to represent the feature vectors of nucleotides [65] , as well as other biological samples (see, e.g., [130] [131] [132] ). Because it has been widely and increasingly used, recently two powerful soft-wares, called \"PseAAC-Builder\" [133] and \"propy\" [134] , were established for generating various special Chou\\'s pseudo-amino acid compositions, in addition to the web-server \"PseAAC\" [135] built in 2008.\\n\\nAccording to a comprehensive review [60] , the general form of PseAAC for a protein sequence P is formulated by\\n\\nwhere the subscript \\uf057 is an integer, and its value as well as the components ( 1, 2, , ) u u \\uf079 \\uf03d\\uf057 will depend on how to extract the desired information from the amino acid sequence of P (cf. Equation (3)). Below, let us describe how to extract useful information to define the components of PseAAC for the NR samples concerned. First, many earlier studies (see, e.g., [136] [137] [138] [139] [140] [141] ) have indicated that the amino acid composition (AAC) of a protein plays an important role in determining its attributes. The AAC contains 20 components with each representing the occurrence frequency of one of the 20 native amino acids in the protein concerned. Thus, such 20 AAC components were used here to define the first 20 elements in Equation (4); i.e., (1) ( 1, 2, , 20) ii fi \\uf079 \\uf03d\\uf03d (5) where f i (1) is the normalized occurrence frequency of the i-th type native amino acid in the nuclear receptor concerned. Since AAC did not contain any sequence order information, the following steps were taken to make up this shortcoming.\\n\\nTo avoid completely losing the local or short-range sequence order information, we considered the approach of dipeptide composition. It contained 20 × 20 = 400 components [142] . Such 400 components were used to define the next 400 elements in Equation (4); i.e., (2) 20 ( 1, 2, , 400) jj fj\\n\\nwhere (2) j f is the normalized occurrence frequency of the j-th dipeptides in the nuclear receptor concerned. To incorporate the global or long-range sequence order information, let us consider the following approach. According to molecular evolution, all biological sequences have developed starting out from a very limited number of ancestral samples. Driven by various evolutionary forces such as mutation, recombination, gene conversion, genetic drift, and selection, they have undergone many changes including changes of single residues, insertions and deletions of several residues [143] , gene doubling, and gene fusion. With the accumulation of these changes over a long period of time, many original similarities between initial and resultant amino acid sequences are gradually faded out, but the corresponding proteins may still share many common attributes [37] , such as having basically the same biological function and residing at a same subcellular location [144, 145] . To extract the sequential evolution information and use it to define the components of Equation (4), the PSSM (Position Specific Scoring Matrix) was used as described below.\\n\\nAccording to Schaffer [146] , the sequence evolution information of a nuclear receptor protein P with L amino acid residues can be expressed by a 20 L\\uf0b4 matrix, as given by\\n\\nwhere (7) were generated by using PSI-BLAST [147] to search the UniProtKB/Swiss-Prot database (The Universal Protein Resource (UniProt); http://www.uniprot.org/) through three iterations with 0.001 as the E-value cutoff for multiple sequence alignment against the sequence of the nuclear receptor concerned. In order to make every element in Equation (7) be scaled from their original score ranges into the region of [0, 1], we performed a conversion through the standard sigmoid function to make it become\\n\\nNow we extract the useful information from Equation (8) \\n\\nMoreover, we used the grey system model approach as elaborated in [68] to further define the next 60 components of Equation (4) ( 1, 2, , 20)\\n\\nIn the above equation, w 1 , w 2 , and w 3 are weight factors, which were all set to 1 in the current study; f j (1) has the same meaning as in Equation (5) \\n\\nwhere \\uf028 \\uf029 \\n\\nand\\n\\nCombining Equations (5), (6), (10) and (12), we found that the total number of the components obtained via the current approach for the PseAAC of Equation (4) \\n\\nand each of the 500 components is given by (1) ( \\n\\nSince the elements in Equations (2) and (4) are well defined, we can now formulate the drug-NR pair by combining the two equations as given by \\uf05b \\uf05d (19) where G represents the drug-NR pair, Å the orthogonal sum, and the 256 + 500 = 756 components are defined by Equations (2) and (18) . For the sake of convenience, let us use x i (i = 1, 2, , 756) to represent the 756 components in Equation (19); i.e., (20) To optimize the prediction quality with a time-saving approach, similar to the treatment [148] [149] [150] , let us convert Equation (20) to\\n\\nwhere the symbol means taking the average of the quantity therein, and SD means the corresponding standard derivation.\\n\\nIn this study, the SVM (support vector machine) was used as the operation engine. SVM has been widely used in the realm of bioinformatics (see, e.g., [62] [63] [64] [151] [152] [153] [154] ). The basic idea of SVM is to transform the data into a high dimensional feature space, and then determine the optimal separating hyperplane using a kernel function. For a brief formulation of SVM and how it works, see the papers [155, 156] ; for more details about SVM, see a monograph [157] .\\n\\nIn this study, the LIBSVM package [158] was used as an implementation of SVM, which can be downloaded from http://www.csie.ntu.edu.tw/~cjlin/libsvm/, the popular radial basis function (RBF) was taken as the kernel function. For the current SVM classifier, there were two uncertain parameters: penalty parameter C and kernel parameter \\uf067 . The method of how to determine the two parameters will be given later.\\n\\nThe predictor obtained via the aforementioned procedure is called iNR-Drug, where \"i\" means identify, and \"NR-Drug\" means the interaction between nuclear receptor and drug compound. To provide an intuitive overall picture, a flowchart is provided in Figure 2 to show the process of how the predictor works in identifying the interactions between nuclear receptors and drug compounds. \\n\\nTo provide a more intuitive and easier-to-understand method to measure the prediction quality, the following set of metrics based on the formulation used by Chou [159] [160] [161] in predicting signal peptides was adopted. According to Chou\\'s formulation, the sensitivity, specificity, overall accuracy, and Matthew\\'s correlation coefficient can be respectively expressed as [62, [65] [66] [67] Sn 1\\n\\nwhere N \\uf02b is the total number of the interactive NR-drug pairs investigated while N \\uf02b \\uf02d the number of the interactive NR-drug pairs incorrectly predicted as the non-interactive NR-drug pairs; N \\uf02d the total number of the non-interactive NR-drug pairs investigated while N \\uf02d \\uf02b the number of the non-interactive NR-drug pairs incorrectly predicted as the interactive NR-drug pairs.\\n\\nAccording to Equation (23) we can easily see the following. When 0 N \\uf02b \\uf02d \\uf03d meaning none of the interactive NR-drug pairs was mispredicted to be a non-interactive NR-drug pair, we have the sensitivity Sn = 1; while NN \\uf02b\\uf02b \\uf02d \\uf03d meaning that all the interactive NR-drug pairs were mispredicted to be the non-interactive NR-drug pairs, we have the sensitivity Sn = 0 . Likewise, when 0 N \\uf02d \\uf02b \\uf03d meaning none of the non-interactive NR-drug pairs was mispredicted, we have the specificity Sp we have MCC = 0 meaning total disagreement between prediction and observation. As we can see from the above discussion, it is much more intuitive and easier to understand when using Equation (23) to examine a predictor for its four metrics, particularly for its Mathew\\'s correlation coefficient. It is instructive to point out that the metrics as defined in Equation (23) are valid for single label systems; for multi-label systems, a set of more complicated metrics should be used as given in [162] .\\n\\nHow to properly test a predictor for its anticipated success rates is very important for its development as well as its potential application value. Generally speaking, the following three cross-validation methods are often used to examine the quality of a predictor and its effectiveness in practical application: independent dataset test, subsampling or K-fold (such as five-fold, seven-fold, or 10-fold) crossover test and jackknife test [163] . However, as elaborated by a penetrating analysis in [164] , considerable arbitrariness exists in the independent dataset test. Also, as demonstrated in [165] , the subsampling (or K-fold crossover validation) test cannot avoid arbitrariness either. Only the jackknife test is the least arbitrary that can always yield a unique result for a given benchmark dataset [73, 74, 156, [166] [167] [168] . Therefore, the jackknife test has been widely recognized and increasingly utilized by investigators to examine the quality of various predictors (see, e.g., [14, 15, 68, 99, 106, 107, 124, 169, 170] ). Accordingly, in this study the jackknife test was also adopted to evaluate the accuracy of the current predictor.\\n\\nAs mentioned above, the SVM operation engine contains two uncertain parameters C and \\uf067 . To find their optimal values, a 2-D grid search was conducted by the jackknife test on the benchmark dataset . The results thus obtained are shown in Figure 3 , from which it can be seen that the iNR-Drug predictor reaches its optimal status when C = 2 3 and 9 2 \\uf067 \\uf02d \\uf03d . The corresponding rates for the four metrics (cf. Equation (23)) are given in Table 1 , where for facilitating comparison, the overall accuracy Acc reported by He et al. [59] on the same benchmark dataset is also given although no results were reported by them for Sn, Sp and MCC. It can be observed from the table that the overall accuracy obtained by iNR-Drug is remarkably higher that of He et al. [59] , and that the rates achieved by iNR-Drug for the other three metrics are also quite higher. These facts indicate that the current predictor not only can yield higher overall prediction accuracy but also is quite stable with low false prediction rates. \\n\\nAs mentioned above (Section 3.2), the jackknife test is the most objective method for examining the quality of a predictor. However, as a demonstration to show how to practically use the current predictor, we took 41 NR-drug pairs from the study by Yamanishi et al. [171] that had been confirmed by experiments as interactive pairs. For such an independent dataset, 34 were correctly identified by iNR-Drug as interactive pairs, i.e., Sn = 34 / 41 = 82.92%, which is quite consistent with the rate of 79.07% achieved by the predictor on the benchmark dataset via the jackknife test as reported in Table 1 .\\n\\nIt is anticipated that the iNR-Drug predictor developed in this paper may become a useful high throughput tool for both basic research and drug development, and that the current approach may be easily extended to study the interactions of drug with other targets as well. Since user-friendly and publicly accessible web-servers represent the future direction for developing practically more useful predictors [98, 172] , a publicly accessible web-server for iNR-Drug was established.\\n\\nFor the convenience of the vast majority of biologists and pharmaceutical scientists, here let us provide a step-by-step guide to show how the users can easily get the desired result by using iNR-Drug web-server without the need to follow the complicated mathematical equations presented in this paper for the process of developing the predictor and its integrity.\\n\\nStep 1. Open the web server at the site http://www.jci-bioinfo.cn/iNR-Drug/ and you will see the top page of the predictor on your computer screen, as shown in Figure 4 . Click on the Read Me button to see a brief introduction about iNR-Drug predictor and the caveat when using it.\\n\\nStep 2. Either type or copy/paste the query NR-drug pairs into the input box at the center of Figure 4 . Each query pair consists of two parts: one is for the nuclear receptor sequence, and the other for the drug. The NR sequence should be in FASTA format, while the drug in the KEGG code beginning with the symbol #. Examples for the query pairs input and the corresponding output can be seen by clicking on the Example button right above the input box. Step 3. Click on the Submit button to see the predicted result. For example, if you use the three query pairs in the Example window as the input, after clicking the Submit button, you will see on your screen that the \"hsa:2099\" NR and the \"D00066\" drug are an interactive pair, and that the \"hsa:2908\" NR and the \"D00088\" drug are also an interactive pair, but that the \"hsa:5468\" NR and the \"D00279\" drug are not an interactive pair. All these results are fully consistent with the experimental observations. It takes about 3 minutes before each of these results is shown on the screen; of course, the more query pairs there is, the more time that is usually needed.\\n\\nStep 4. Click on the Citation button to find the relevant paper that documents the detailed development and algorithm of iNR-Durg.\\n\\nStep 5. Click on the Data button to download the benchmark dataset used to train and test the iNR-Durg predictor.\\n\\nStep 6. The program code is also available by clicking the button download on the lower panel of Figure 4 .',\n",
              " 'document_id': 1572,\n",
              " 'id': 1635,\n",
              " 'is_impossible': False,\n",
              " 'question': 'What can nuclear receptors regulate?'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtzXHZiu9cs0"
      },
      "source": [
        "We can see the training, validation and test sets all have a column for the context, the question and the answers to those questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3EtYfeHIrIz"
      },
      "source": [
        "To access an actual element, you need to select a split first, then give an index:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIA71hFd9cs1"
      },
      "source": [
        "We can see the answers are indicated by their start position in the text (here at character 515) and their full text, which is a substring of the context as we mentioned above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHUmphG3IrI3"
      },
      "source": [
        "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset (automatically decoding the labels in passing)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3j8APAoIrI3"
      },
      "source": [
        "from datasets import ClassLabel, Sequence\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_random_elements(dataset, num_examples=1):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "    \n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
        "    display(HTML(df.to_html()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SZy5tRB_IrI7",
        "scrolled": true,
        "outputId": "65a759a8-d446-4553-eed3-8cd135c604dd"
      },
      "source": [
        "show_random_elements(datasets[\"train\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>answers</th>\n",
              "      <th>context</th>\n",
              "      <th>document_id</th>\n",
              "      <th>id</th>\n",
              "      <th>is_impossible</th>\n",
              "      <th>question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{'answer_start': [28444], 'text': ['Rhinolophus bats seem to harbor a wide diversity of CoVs']}</td>\n",
              "      <td>Characterization of a New Member of Alphacoronavirus with Unique Genomic Features in Rhinolophus Bats\\n\\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC6521148/\\n\\nSHA: ee14de143337eec0e9708f8139bfac2b7b8fdd27\\n\\nAuthors: Wang, Ning; Luo, Chuming; Liu, Haizhou; Yang, Xinglou; Hu, Ben; Zhang, Wei; Li, Bei; Zhu, Yan; Zhu, Guangjian; Shen, Xurui; Peng, Cheng; Shi, Zhengli\\nDate: 2019-04-24\\nDOI: 10.3390/v11040379\\nLicense: cc-by\\n\\nAbstract: Bats have been identified as a natural reservoir of a variety of coronaviruses (CoVs). Several of them have caused diseases in humans and domestic animals by interspecies transmission. Considering the diversity of bat coronaviruses, bat species and populations, we expect to discover more bat CoVs through virus surveillance. In this study, we described a new member of alphaCoV (BtCoV/Rh/YN2012) in bats with unique genome features. Unique accessory genes, ORF4a and ORF4b were found between the spike gene and the envelope gene, while ORF8 gene was found downstream of the nucleocapsid gene. All the putative genes were further confirmed by reverse-transcription analyses. One unique gene at the 3’ end of the BtCoV/Rh/YN2012 genome, ORF9, exhibits ~30% amino acid identity to ORF7a of the SARS-related coronavirus. Functional analysis showed ORF4a protein can activate IFN-β production, whereas ORF3a can regulate NF-κB production. We also screened the spike-mediated virus entry using the spike-pseudotyped retroviruses system, although failed to find any fully permissive cells. Our results expand the knowledge on the genetic diversity of bat coronaviruses. Continuous screening of bat viruses will help us further understand the important role played by bats in coronavirus evolution and transmission.\\n\\nText: Members of the Coronaviridae family are enveloped, non-segmented, positive-strand RNA viruses with genome sizes ranging from 26-32 kb [1] . These viruses are classified into two subfamilies: Letovirinae, which contains the only genus: Alphaletovirus; and Orthocoronavirinae (CoV), which consists of alpha, beta, gamma, and deltacoronaviruses (CoVs) [2, 3] . Alpha and betacoronaviruses mainly infect mammals and cause human and animal diseases. Gamma-and delta-CoVs mainly infect birds, but some can also infect mammals [4, 5] . Six human CoVs (HCoVs) are known to cause human diseases. HCoV-HKU1, HCoV-OC43, HCoV-229E, and HCoV-NL63 commonly cause mild respiratory illness or asymptomatic infection; however, severe acute respiratory syndrome coronavirus (SARS-CoV) and\\n\\nAll sampling procedures were performed by veterinarians, with approval from Animal Ethics Committee of the Wuhan Institute of Virology (WIVH5210201). The study was conducted in accordance with the Guide for the Care and Use of Wild Mammals in Research of the People's Republic of China.\\n\\nBat fecal swab and pellet samples were collected from November 2004 to November 2014 in different seasons in Southern China, as described previously [16] .\\n\\nViral RNA was extracted from 200 µL of fecal swab or pellet samples using the High Pure Viral RNA Kit (Roche Diagnostics GmbH, Mannheim, Germany) as per the manufacturer's instructions. RNA was eluted in 50 µL of elution buffer, aliquoted, and stored at -80 • C. One-step hemi-nested reverse-transcription (RT-) PCR (Invitrogen, San Diego, CA, USA) was employed to detect coronavirus, as previously described [17, 18] .\\n\\nTo confirm the bat species of an individual sample, we PCR amplified the cytochrome b (Cytob) and/or NADH dehydrogenase subunit 1 (ND1) gene using DNA extracted from the feces or swabs [19, 20] . The gene sequences were assembled excluding the primer sequences. BLASTN was used to identify host species based on the most closely related sequences with the highest query coverage and a minimum identity of 95%.\\n\\nFull genomic sequences were determined by one-step PCR (Invitrogen, San Diego, CA, USA) amplification with degenerate primers (Table S1 ) designed on the basis of multiple alignments of available alpha-CoV sequences deposited in GenBank or amplified with SuperScript IV Reverse Transcriptase (Invitrogen) and Expand Long Template PCR System (Roche Diagnostics GmbH, Mannheim, Germany) with specific primers (primer sequences are available upon request). Sequences of the 5' and 3' genomic ends were obtained by 5' and 3' rapid amplification of cDNA ends (SMARTer Viruses 2019, 11, 379 3 of 19 RACE 5'/3' Kit; Clontech, Mountain View, CA, USA), respectively. PCR products were gel-purified and subjected directly to sequencing. PCR products over 5kb were subjected to deep sequencing using Hiseq2500 system. For some fragments, the PCR products were cloned into the pGEM-T Easy Vector (Promega, Madison, WI, USA) for sequencing. At least five independent clones were sequenced to obtain a consensus sequence.\\n\\nThe Next Generation Sequencing (NGS) data were filtered and mapped to the reference sequence of BatCoV HKU10 (GenBank accession number NC_018871) using Geneious 7.1.8 [21] . Genomes were preliminarily assembled using DNAStar lasergene V7 (DNAStar, Madison, WI, USA). Putative open reading frames (ORFs) were predicted using NCBI's ORF finder (https://www.ncbi.nlm.nih.gov/ orffinder/) with a minimal ORF length of 150 nt, followed by manual inspection. The sequences of the 5' untranslated region (5'-UTR) and 3'-UTR were defined, and the leader sequence, the leader and body transcriptional regulatory sequence (TRS) were identified as previously described [22] . The cleavage of the 16 nonstructural proteins coded by ORF1ab was determined by alignment of aa sequences of other CoVs and the recognition pattern of the 3C-like proteinase and papain-like proteinase. Phylogenetic trees based on nt or aa sequences were constructed using the maximum likelihood algorithm with bootstrap values determined by 1000 replicates in the MEGA 6 software package [23] . Full-length genome sequences obtained in this study were aligned with those of previously reported alpha-CoVs using MUSCLE [24] . The aligned sequences were scanned for recombination events by using Recombination Detection Program [25] . Potential recombination events as suggested by strong p-values (&lt;10 -20 ) were confirmed using similarity plot and bootscan analyses implemented in Simplot 3.5.1 [26] . The number of synonymous substitutions per synonymous site, Ks, and the number of nonsynonymous substitutions per nonsynonymous site, Ka, for each coding region were calculated using the Ka/Ks calculation tool of the Norwegian Bioinformatics Platform (http://services.cbu.uib.no/tools/kaks) with default parameters [27] . The protein homology detection was analyzed using HHpred (https://toolkit.tuebingen.mpg.de/#/tools/hhpred) with default parameters [28] .\\n\\nA set of nested RT-PCRs was employed to determine the presence of viral subgenomic mRNAs in the CoV-positive samples [29] . Forward primers were designed targeting the leader sequence at the 5'-end of the complete genome, while reverse primers were designed within the ORFs. Specific and suspected amplicons of expected sizes were purified and then cloned into the pGEM-T Easy vector for sequencing.\\n\\nBat primary or immortalized cells (Rhinolophus sinicus kidney immortalized cells, RsKT; Rhinolophus sinicus Lung primary cells, RsLu4323; Rhinolophus sinicus brain immortalized cells, RsBrT; Rhinolophus affinis kidney primary cells, RaK4324; Rousettus leschenaultii Kidney immortalized cells, RlKT; Hipposideros pratti lung immortalized cells, HpLuT) generated in our laboratory were all cultured in DMEM/F12 with 15% FBS. Pteropus alecto kidney cells (Paki) was maintained in DMEM/F12 supplemented with 10% FBS. Other cells were maintained according to the recommendations of American Type Culture Collection (ATCC, www.atcc.org).\\n\\nThe putative accessory genes of the newly detected virus were generated by RT-PCR from viral RNA extracted from fecal samples, as described previously [30] . The influenza virus NS1 plasmid was generated in our lab [31] . The human bocavirus (HBoV) VP2 plasmid was kindly provided by prof. Hanzhong Wang of the Wuhan Institute of Virology, Chinese Academy of Sciences. SARS-CoV ORF7a was synthesized by Sangon Biotech. The transfections were performed with Lipofectamine 3000 Reagent (Life Technologies). Expression of these accessory genes were analyzed by Western blotting using an mAb (Roche Diagnostics GmbH, Mannheim, Germany) against the HA tag. \\n\\nThe virus isolation was performed as previously described [12] . Briefly, fecal supernatant was acquired via gradient centrifugation and then added to Vero E6 cells, 1:10 diluted in DMEM. After incubation at 37°C for 1 h the inoculum was replaced by fresh DMEM containing 2% FBS and the antibiotic-antimycotic (Gibco, Grand Island, NY, USA). Three blind passages were carried out. Cells were checked daily for cytopathic effect. Both culture supernatant and cell pellet were examined for CoV by RT-PCR [17] .\\n\\nApoptosis was analyzed as previously described [18] . Briefly, 293T cells in 12-well plates were transfected with 3 µg of expression plasmid or empty vector, and the cells were collected 24 h post transfection. Apoptosis was detected by flow cytometry using by the Annexin V-FITC/PI Apoptosis Detection Kit (YEASEN, Shanghai, China) following the manufacturer's instructions. Annexin-V-positive and PI-negative cells were considered to be in the early apoptotic phase and those stained for both Annexin V and PI were deemed to undergo late apoptosis or necrosis. All experiments were repeated three times. Student's t-test was used to evaluate the data, with p &lt; 0.05 considered significant.\\n\\nHEK 293T cells were seeded in 24-well plates and then co-transfected with reporter plasmids (pRL-TK and pIFN-βIFN-or pNF-κB-Luc) [30] , as well as plasmids expressing accessory genes, empty vector plasmid pcAGGS, influenza virus NS1 [32] , SARS-CoV ORF7a [33] , or HBoV VP2 [34] . At 24 h post transfection, cells were treated with Sendai virus (SeV) (100 hemagglutinin units [HAU]/mL) or human tumor necrosis factor alpha (TNF-α; R&amp;D system) for 6 h to activate IFNβ or NF-κB, respectively. Cell lysates were prepared, and luciferase activity was measured using the dual-luciferase assay kit (Promega, Madison, WI, USA) according to the manufacturer's instructions.\\n\\nRetroviruses pseudotyped with BtCoV/Rh/YN2012 RsYN1, RsYN3, RaGD, or MERS-CoV spike, or no spike (mock) were used to infect human, bat or other mammalian cells in 96-well plates. The pseudovirus particles were confirmed with Western blotting and negative-staining electromicroscopy. The production process, measurements of infection and luciferase activity were conducted, as described previously [35, 36] .\\n\\nThe complete genome nucleotide sequences of BtCoV/Rh/YN2012 strains RsYN1, RsYN2, RsYN3, and RaGD obtained in this study have been submitted to the GenBank under MG916901 to MG916904.\\n\\nThe surveillance was performed between November 2004 to November 2014 in 19 provinces of China. In total, 2061 fecal samples were collected from at least 12 Rhinolophus bat species ( Figure 1A ). CoVs were detected in 209 of these samples ( Figure 1B and Table 1 ). Partial RdRp sequences suggested the presence of at least 8 different CoVs. Five of these viruses are related to known species: Mi-BatCoV 1 (&gt;94% nt identity), Mi-BatCoV HKU8 [37] (&gt;93% nt identity), BtRf-AlphaCoV/HuB2013 [11] (&gt;99% nt identity), SARSr-CoV [38] (&gt;89% nt identity), and HKU2-related CoV [39] (&gt;85% nt identity). While the other three CoV sequences showed less than 83% nt identity to known CoV species. These three viruses should represent novel CoV species. Virus isolation was performed as previously described [12] , but was not successful. identity). While the other three CoV sequences showed less than 83% nt identity to known CoV species. These three viruses should represent novel CoV species. Virus isolation was performed as previously described [12] , but was not successful. \\n\\nWe next characterized a novel alpha-CoV, BtCoV/Rh/YN2012. It was detected in 3 R.affinis and 6 R.sinicus, respectively. Based on the sequences, we defined three genotypes, which represented by RsYN1, RsYN3, and RaGD, respectively. Strain RsYN2 was classified into the RsYN3 genotype. Four full-length genomes were obtained. Three of them were from R.sinicus (Strain RsYN1, RsYN2, and RsYN3), while the other one was from R.affinis (Strain RaGD). The sizes of these 4 genomes are between 28,715 to 29,102, with G+C contents between 39.0% to 41.3%. The genomes exhibit similar structures and transcription regulatory sequences (TRS) that are identical to those of other alpha-CoVs ( Figure 2 and Table 2 ). Exceptions including three additional ORFs (ORF3b, ORF4a and ORF4b) were observed. All the 4 strains have ORF4a &amp; ORF4b, while only strain RsYN1 has ORF3b.\\n\\nThe replicase gene, ORF1ab, occupies~20.4 kb of the genome. The replicase gene, ORF1ab, occupies~20.4 kb of the genome. It encodes polyproteins 1a and 1ab, which could be cleaved into 16 non-structural proteins (Nsp1-Nsp16). The 3'-end of the cleavage sites recognized by 3C-like proteinase (Nsp4-Nsp10, Nsp12-Nsp16) and papain-like proteinase (Nsp1-Nsp3) were confirmed. The proteins including Nsp3 (papain-like 2 proteas, PL2pro), Nsp5 (chymotrypsin-like protease, 3CLpro), Nsp12 (RdRp), Nsp13 (helicase), and other proteins of unknown function ( Table 3 ). The 7 concatenated domains of polyprotein 1 shared &lt;90% aa sequence identity with those of other known alpha-CoVs ( Table 2 ), suggesting that these viruses represent a novel CoV species within the alpha-CoV. The closest assigned CoV species to BtCoV/Rh/YN2012 are BtCoV-HKU10 and BtRf-AlphaCoV/Hub2013. The three strains from Yunnan Province were clustered into two genotypes (83% genome identity) correlated to their sampling location. The third genotype represented by strain RaGD was isolated to strains found in Yunnan (&lt;75.4% genome identity). We then examined the individual genes ( Table 2) . All of the genes showed low aa sequence identity to known CoVs. The four strains of BtCoV/Rh/YN2012 showed genetic diversity among all different genes except ORF1ab (&gt;83.7% aa identity). Notably, the spike proteins are highly divergent among these strains. Other structure proteins (E, M, and N) are more conserved than the spike and other accessory proteins. Comparing the accessory genes among these four strains revealed that the strains of the same genotype shared a 100% identical ORF3a. However, the proteins encoded by ORF3as were highly divergent among different genotypes (&lt;65% aa identity). The putative accessory genes were also BLASTed against GenBank records. Most accessory genes have no homologues in GenBank-database, except for ORF3a (52.0-55.5% aa identity with BatCoV HKU10 ORF3) and ORF9 (28.1-32.0% aa identity with SARSr-CoV ORF7a). We analyzed the protein homology with HHpred software. The results showed that ORF9s and SARS-CoV OR7a are homologues (possibility: 100%, E value &lt;10 −48 ). We further screened the genomes for potential recombination evidence. No significant recombination breakpoint was detected by bootscan analysis.\\n\\nTo confirm the presence of subgenomic RNA, we designed a set of primers targeting all the predicted ORFs as described. The amplicons were firstly confirmed via agarose-gel electrophoresis and then sequencing ( Figure 3 and Table 2 ). The sequences showed that all the ORFs, except ORF4b, had preceding TRS. Hence, the ORF4b may be translated from bicistronic mRNAs. In RsYN1, an additional subgenomic RNA starting inside the ORF3a was found through sequencing, which led to a unique ORF3b. \\n\\nTo confirm the presence of subgenomic RNA, we designed a set of primers targeting all the predicted ORFs as described. The amplicons were firstly confirmed via agarose-gel electrophoresis and then sequencing ( Figure 3 and Table 2 ). The sequences showed that all the ORFs, except ORF4b, had preceding TRS. Hence, the ORF4b may be translated from bicistronic mRNAs. In RsYN1, an additional subgenomic RNA starting inside the ORF3a was found through sequencing, which led to a unique ORF3b. \\n\\nPhylogenetic trees were constructed using the aa sequences of RdRp and S of BtCoV/Rh/YN2012 and other representative CoVs (Figure 4) . In both trees, all BtCoV/Rh/YN2012 were clustered together and formed a distinct lineage to other known coronavirus species. Two distinct sublineages were observed within BtCoV/Rh/YN2012. One was from Ra sampled in Guangdong, while the other was from Rs sampled in Yunnan Among the strains from Yunnan, RsYN2 and RsYN3 were clustered together, while RsYN1 was isolated. The topology of these four strains was correlated to the sampling location. The relatively long branches reflect a high diversity among these strains, indicating a long independent evolution history. \\n\\nPhylogenetic trees were constructed using the aa sequences of RdRp and S of BtCoV/Rh/YN2012 and other representative CoVs (Figure 4) . In both trees, all BtCoV/Rh/YN2012 were clustered together and formed a distinct lineage to other known coronavirus species. Two distinct sublineages were observed within BtCoV/Rh/YN2012. One was from Ra sampled in Guangdong, while the other was from Rs sampled in Yunnan Among the strains from Yunnan, RsYN2 and RsYN3 were clustered together, while RsYN1 was isolated. The topology of these four strains was correlated to the sampling location. The relatively long branches reflect a high diversity among these strains, indicating a long independent evolution history. \\n\\nPhylogenetic trees were constructed using the aa sequences of RdRp and S of BtCoV/Rh/YN2012 and other representative CoVs (Figure 4) . In both trees, all BtCoV/Rh/YN2012 were clustered together and formed a distinct lineage to other known coronavirus species. Two distinct sublineages were observed within BtCoV/Rh/YN2012. One was from Ra sampled in Guangdong, while the other was from Rs sampled in Yunnan Among the strains from Yunnan, RsYN2 and RsYN3 were clustered together, while RsYN1 was isolated. The topology of these four strains was correlated to the sampling location. The relatively long branches reflect a high diversity among these strains, indicating a long independent evolution history. \\n\\nThe Ka/Ks ratios (Ks is the number of synonymous substitutions per synonymous sites and Ka is the number of nonsynonymous substitutions per nonsynonymous site) were calculated for all genes. The Ka/Ks ratios for most of the genes were generally low, which indicates these genes were under purified selection. However, the Ka/Ks ratios of ORF4a, ORF4b, and ORF9 (0.727, 0.623, and 0.843, respectively) were significantly higher than those of other ORFs (Table 4 ). For further selection pressure evaluation of the ORF4a and ORF4b gene, we sequenced another four ORF4a and ORF4b genes (strain Rs4223, Rs4236, Rs4240, and Ra13576 was shown in Figure 1B \\n\\nAs SARS-CoV ORF7a was reported to induce apoptosis, we conducted apoptosis analysis on BtCoV/Rh/YN2012 ORF9, a~30% aa identity homologue of SARSr-CoV ORF7a. We transiently transfected ORF9 of BtCoV/Rh/YN2012 into HEK293T cells to examine whether this ORF9 triggers apoptosis. Western blot was performed to confirm the expression of ORF9s and SARS-CoV ORF7a ( Figure S1 ). ORF9 couldn't induce apoptosis as the ORF7a of SARS-CoV Tor2 ( Figure S2 ). The results indicated that BtCoV/Rh/YN2012 ORF9 was not involved in apoptosis induction.\\n\\nTo determine whether these accessory proteins modulate IFN induction, we transfected reporter plasmids (pIFNβ-Luc and pRL-TK) and expression plasmids to 293T cells. All the cells over-expressing the accessory genes, as well as influenza virus NS1 (strain PR8), HBoV VP2, or empty vector were tested for luciferase activity after SeV infection. Luciferase activity stimulated by SeV was remarkably higher than that without SeV treatment as expected. Influenza virus NS1 inhibits the expression from IFN promoter, while HBoV VP2 activate the expression. Compared to those controls, the ORF4a proteins exhibit an active effect as HBoV VP2 ( Figure 5A ). Other accessory proteins showed no effect on IFN production ( Figure S3 ). Expression of these accessory genes were confirmed by Western blot ( Figure S1 ). was remarkably higher than that without SeV treatment as expected. Influenza virus NS1 inhibits the expression from IFN promoter, while HBoV VP2 activate the expression. Compared to those controls, the ORF4a proteins exhibit an active effect as HBoV VP2 ( Figure 5A ). Other accessory proteins showed no effect on IFN production ( Figure S3 ). Expression of these accessory genes were confirmed by Western blot (Figure S1 ). Samples were collected at 6 h postinfection, followed by dual-luciferase assay. The results were expressed as the firefly luciferase value normalized to that of Renilla luciferase. (B) ORF3a protein activate NF-κB. 293T cells were transfected with 100 ng pNF-κB-Luc, 10 ng pRL-TK, empty vector (500 ng), an NS1-expressing plasmid (500 ng), a SARS-CoV ORF7a-expressing plasmid (500 ng), or ORF3a-expressing plasmids (500 ng). After 24 h, the cells were treated with TNF-α. Dual-luciferase activity was determined after 6 h. The results were expressed as the firefly luciferase activity normalized to that of Renilla luciferase. The experiments were performed three times independently. Data are representative of at least three independent experiments, with each determination performed in triplicate (mean ± SD of fold change). Asterisks indicate significant differences between groups (compared with Empty vector-NC, p &lt; 0.05, as determined by student t test).\\n\\nNF-κB plays an important role in regulating the immune response to viral infection and is also a key factor frequently targeted by viruses for taking over the host cell. In this study, we tested if these accessory proteins could modulate NF-κB. 293T cells were co-transfected with reporter Samples were collected at 6 h postinfection, followed by dual-luciferase assay. The results were expressed as the firefly luciferase value normalized to that of Renilla luciferase. (B) ORF3a protein activate NF-κB. 293T cells were transfected with 100 ng pNF-κB-Luc, 10 ng pRL-TK, empty vector (500 ng), an NS1-expressing plasmid (500 ng), a SARS-CoV ORF7a-expressing plasmid (500 ng), or ORF3a-expressing plasmids (500 ng). After 24 h, the cells were treated with TNF-α. Dual-luciferase activity was determined after 6 h. The results were expressed as the firefly luciferase activity normalized to that of Renilla luciferase. The experiments were performed three times independently. Data are representative of at least three independent experiments, with each determination performed in triplicate (mean ± SD of fold change). Asterisks indicate significant differences between groups (compared with Empty vector-NC, p &lt; 0.05, as determined by student t test).\\n\\nNF-κB plays an important role in regulating the immune response to viral infection and is also a key factor frequently targeted by viruses for taking over the host cell. In this study, we tested if these accessory proteins could modulate NF-κB. 293T cells were co-transfected with reporter plasmids (pNF-κB-Luc and pRL-TK), as well as accessory protein-expressing plasmids, or controls (empty vector, NS1, SARS-CoV Tor2-ORF7a). The cells were mock treated or treated with TNF-α for 6 h at 24 h post-transfection. The luciferase activity was determined. RsYN1-ORF3a and RaGD-ORF3a activated NF-κB as SARS-CoV ORF7a, whereas RsYN2-ORF3a inhibited NF-κB as NS1 ( Figure 5B ). Expressions of ORF3as were confirmed with Western blot ( Figure S1 ). Other accessory proteins did not modulate NF-κB production ( Figure S4 ).\\n\\nTo understand the infectivity of these newly detected BtCoV/Rh/YN2012, we selected the RsYN1, RsYN3 and RaGD spike proteins for spike-mediated pseudovirus entry studies. Both Western blot analysis and negative-staining electron microscopy observation confirmed the preparation of BtCoV/Rh/YN2012 successfully ( Figure S5 ). A total of 11 human cell lines, 8 bat cells, and 9 other mammal cell lines were tested, and no strong positive was found (Table S2) .\\n\\nIn this study, a novel alpha-CoV species, BtCoV/Rh/YN2012, was identified in two Rhinolophus species. The 4 strains with full-length genome were sequences. The 7 conserved replicase domains of these viruses possessed &lt;90% aa sequence identity to those of other known alpha-CoVs, which defines a new species in accordance with the ICTV taxonomy standard [42] . These novel alpha-CoVs showed high genetic diversity in their structural and non-structural genes. Strain RaGD from R. affinis, collected in Guangdong province, formed a divergent independent branch from the other 3 strains from R. sinicus, sampled in Yunnan Province, indicating an independent evolution process associated with geographic isolation and host restrain. Though collected from same province, these three virus strains formed two genotypes correlated to sampling locations. These two genotypes had low genome sequence identity, especially in the S gene and accessory genes. Considering the remote geographic location of the host bat habitat, the host tropism, and the virus diversity, we suppose BtCoV/Rh/YN2012 may have spread in these two provinces with a long history of circulation in their natural reservoir, Rhinolophus bats. With the sequence evidence, we suppose that these viruses are still rapidly evolving.\\n\\nOur study revealed that BtCoV/Rh/YN2012 has a unique genome structure compared to other alpha-CoVs. First, novel accessory genes, which had no homologues, were identified in the genomes. Second, multiple TRSs were found between S and E genes while other alphacoronavirus only had one TRS there. These TRSs precede ORF3a, ORF3b (only in RsYN1), and ORF4a/b respectively. Third, accessory gene ORF9 showed homology with those of other known CoV species in another coronavirus genus, especially with accessory genes from SARSr-CoV.\\n\\nAccessory genes are usually involved in virus-host interactions during CoV infection [43] . In most CoVs, accessory genes are dispensable for virus replication. However, an intact 3c gene of feline CoV was required for viral replication in the gut [44] [45] [46] . Deletion of the genus-specific genes in mouse hepatitis virus led to a reduction in virulence [47] . SARS-CoV ORF7a, which was identified to be involved in the suppression of RNA silencing [48] , inhibition of cellular protein synthesis [49] , cell-cycle blockage [50] , and apoptosis induction [51, 52] . In this study, we found that BtCoV/Rh/YN2012 ORF9 shares~30% aa sequence identity with SARS-CoV ORF7a. Interestingly, BtCoV/Rh/YN2012 and SARSr-CoV were both detected in R. sinicus from the same cave. We suppose that SARS-CoV and BtCoV/Rh/YN2012 may have acquired ORF7a or ORF9 from a common ancestor through genome recombination or horizontal gene transfer. Whereas, ORF9 of BtCoV/Rh/YN2012 failed to induce apoptosis or activate NF-κB production, these differences may be induced by the divergent evolution of these proteins in different pressure.\\n\\nThough different BtCoV/Rh/YN2012 ORF4a share &lt;64.4% amino acid identity, all of them could activate IFN-β. ORF3a from RsYN1 and RaGD upregulated NF-κB, but the homologue from RsYN2 downregulated NF-κB expression. These differences may be caused by amino acid sequence variations and may contribute to a viruses' pathogenicity with a different pathway.\\n\\nThough lacking of intestinal cell lines from the natural host of BtCoV/Rh/YN2012, we screened the cell tropism of their spike protein through pseudotyped retrovirus entry with human, bat and other mammalian cell lines. Most of cell lines screened were unsusceptible to BtCoV/Rh/YN2012, indicating a low risk of interspecies transmission to human and other animals. Multiple reasons may lead to failed infection of coronavirus spike-pseudotyped retrovirus system, including receptor absence in target cells, failed recognition to the receptor homologue from non-host species, maladaptation in non-host cells during the spike maturation or virus entry, or the limitation of retrovirus system in stimulating coronavirus entry. The weak infectivity of RsYN1 pseudotyped retrovirus in Huh-7 cells could be explained by the binding of spike protein to polysaccharide secreted to the surface. The assumption needs to be further confirmed by experiments.\\n\\nOur long-term surveillances suggest that Rhinolophus bats seem to harbor a wide diversity of CoVs. Coincidently, the two highly pathogenic agents, SARS-CoV and Rh-BatCoV HKU2 both originated from Rhinolophus bats. Considering the diversity of CoVs carried by this bat genus and their wide geographical distribution, there may be a low risk of spillover of these viruses to other animals and humans. Long-term surveillances and pathogenesis studies will help to prevent future human and animal diseases caused by these bat CoVs.\\n\\nSupplementary Materials: The following are available online at http://www.mdpi.com/1999-4915/11/4/379/s1, Figure S1 : western blot analysis of the expression of accessory proteins. Figure S2 : Apoptosis analysis of ORF9 proteins of BtCoV/Rh/YN2012. Figure S3 : Functional analysis of ORF3a, ORF3b, ORF4b, ORF8 and ORF9 proteins on the production of Type I interferon. Figure S4 : Functional analysis of ORF3b, ORF4a, ORF4b, ORF8 and ORF9 proteins on the production of NF-κB. Figure S5 : Characteristic of BtCoV/Rh/YN2012 spike mediated pseudovirus. Table S1 : General primers for AlphaCoVs genome sequencing. Table S2 : Primers for the detection of viral sugbenomic mRNAs. Table S3</td>\n",
              "      <td>1576</td>\n",
              "      <td>3686</td>\n",
              "      <td>False</td>\n",
              "      <td>What is the conclusion of the coronavirus long-term surveillance studies?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "## Preprocessing the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVx71GdAIrJH"
      },
      "source": [
        "Before we can feed those texts to our model, we need to preprocess them. This is done by a 🤗 Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
        "\n",
        "To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n",
        "\n",
        "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
        "- we download the vocabulary used when pretraining this specific checkpoint.\n",
        "\n",
        "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262,
          "referenced_widgets": [
            "89397a070a364f39b7bebf8217ad6302",
            "71d07444f98a4ba88db9aa2b36d06d03",
            "d4cf26cd549749fc8cdc28625cf734d4",
            "77e1ab9debc1482a8ff75e949f3b2d72",
            "9e19357ed2bc43feb5e2712a36635fc7",
            "610f08762e85494889126e893711722d",
            "6f6c16f7573c4d33af74ad764f825d10",
            "1772a435ec624f369b04d32fb5bee060",
            "7b51e48317dd449b9d81293886921935",
            "9e15846cd4634c34ab7efc63bd9f99a9",
            "97129f3a6cce4423800237fe7a45e2a7",
            "16a440cee8cc48a091d29ffcfa3e230b",
            "62b17418670041e9b8487d9e921d9363",
            "109093c53bc940cba07227b695e1381f",
            "e607cfe24e8e45ebab988b26dd35718d",
            "838b5ddb8913428589d3a874e815d44f",
            "bac3d706bef242adb91c920d1718e374",
            "d192adc1f27d468e9a6cdade8343d2ef",
            "f0442257f23047db9177b89325918b97",
            "b3d6707cb3914af2829a2923257ffe6b",
            "06cccb63e3c548718e5e35852561c830",
            "b7f35b0978bf4f2ab3f48d4b1478d05a",
            "0b48ad8149e84916a67f0731c8a9bcc6",
            "50afd1d590b342449903c9d904f20c4a",
            "cee17275fa7045f5aa931e6d30c2ed53",
            "7ae2880bd5f7429287dfadeb39a80562",
            "38165347fc38458e8a75e89e5df68682",
            "f35749c87d29483789417fd06967d515",
            "83820ce4a5f7403eb5439ae0b455366d",
            "5be5de12f3a347aa8e9920b0d6402af3",
            "9c979b3347734e7db714ed235c443338",
            "53bfb1f33ab94c8782987dad587074ee",
            "c3c193688f12460ea978db4dbc909ebb",
            "355d49d957ae4035850578dfc1b05093",
            "ac7fad6dd4774ab08656bca8733b04ee",
            "e70d425c11af45baa9fff194eb9b8520",
            "10a2f525393744e6bd2576d56c25889f",
            "ce96794a5a6248d3875945e7779b0eab",
            "04e14eb14ae84ce19121426e7ee9c932",
            "16b351bf68bb43eda4dc6ff41ab06dc7"
          ]
        },
        "id": "eXNLu_-nIrJI",
        "outputId": "605338ed-7e1d-4e05-aaa7-5796ac92014a"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "    \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89397a070a364f39b7bebf8217ad6302",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1265.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b51e48317dd449b9d81293886921935",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898822.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bac3d706bef242adb91c920d1718e374",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cee17275fa7045f5aa931e6d30c2ed53",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=150.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3c193688f12460ea978db4dbc909ebb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=190.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl6IidfdIrJK"
      },
      "source": [
        "The following assertion ensures that our tokenizer is a fast tokenizers (backed by Rust) from the 🤗 Tokenizers library. Those fast tokenizers are available for almost all models, and we will need some of the special features they have for our preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x1DUx_l9cs3"
      },
      "source": [
        "import transformers\n",
        "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vdFCDTK9cs4"
      },
      "source": [
        "You can check which type of models have a fast tokenizer available and which don't on the [big table of models](https://huggingface.co/transformers/index.html#bigtable)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rowT4iCLIrJK"
      },
      "source": [
        "You can directly call this tokenizer on two sentences (one for the answer, one for the context):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5hBlsrHIrJL",
        "outputId": "99674f7e-eb1c-416d-ddf7-e24514a09a00"
      },
      "source": [
        "tokenizer(\"What is your name?\", \"My name is Sylvain.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [0, 2264, 16, 110, 766, 116, 2, 2, 2387, 766, 16, 28856, 1851, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcKkiyXZ9cs4"
      },
      "source": [
        "Depending on the model you selected, you will see different keys in the dictionary returned by the cell above. They don't matter much for what we're doing here (just know they are required by the model we will instantiate later), you can learn more about them in [this tutorial](https://huggingface.co/transformers/preprocessing.html) if you're interested.\n",
        "\n",
        "Now one specific thing for the preprocessing in question answering is how to deal with very long documents. We usually truncate them in other tasks, when they are longer than the model maximum sentence length, but here, removing part of the the context might result in losing the answer we are looking for. To deal with this, we will allow one (long) example in our dataset to give several input features, each of length shorter than the maximum length of the model (or the one we set as a hyper-parameter). Also, just in case the answer lies at the point we split a long context, we allow some overlap between the features we generate controlled by the hyper-parameter `doc_stride`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uO0kcG8j9cs5"
      },
      "source": [
        "max_length = 500 # The maximum length of a feature (question and context)\n",
        "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Chc24MxJ9cs5"
      },
      "source": [
        "Let's find one long example in our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiPyUS3x9cs5",
        "outputId": "fbf6720f-28ef-47c2-9a69-3bd1eb25df17"
      },
      "source": [
        "for i, example in enumerate(datasets[\"train\"]):\n",
        "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > 384:\n",
        "        break\n",
        "example = datasets[\"train\"][i]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (5908 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdv-h7ZC9cs6"
      },
      "source": [
        "Without any truncation, we get the following length for the input IDs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfmf9naV9cs6",
        "outputId": "e45a30d4-67d2-4b58-8f5c-67439b36f85a"
      },
      "source": [
        "len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5908"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLkqnyX39cs7"
      },
      "source": [
        "Now, if we just truncate, we will lose information (and possibly the answer to our question):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5TAOdp19cs7",
        "outputId": "8d82a019-7c64-4bff-a698-dbe734dcd053"
      },
      "source": [
        "len(tokenizer(example[\"question\"], example[\"context\"], max_length=max_length, truncation=\"only_second\")[\"input_ids\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmkoAhpS9cs8"
      },
      "source": [
        "Note that we never want to truncate the question, only the context, else the `only_second` truncation picked. Now, our tokenizer can automatically return us a list of features capped by a certain maximum length, with the overlap we talked above, we just have to tell it with `return_overflowing_tokens=True` and by passing the stride:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_GZnIKI9cs9"
      },
      "source": [
        "tokenized_example = tokenizer(\n",
        "    example[\"question\"],\n",
        "    example[\"context\"],\n",
        "    max_length=max_length,\n",
        "    truncation=\"only_second\",\n",
        "    return_overflowing_tokens=True,\n",
        "    stride=doc_stride\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jecy03IX9cs9"
      },
      "source": [
        "Now we don't have one list of `input_ids`, but several: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shaSdrOx9cs9",
        "outputId": "6262f274-e056-475a-f14f-f0d4a69dcf5b"
      },
      "source": [
        "[len(x) for x in tokenized_example[\"input_ids\"]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[500,\n",
              " 500,\n",
              " 500,\n",
              " 500,\n",
              " 500,\n",
              " 500,\n",
              " 500,\n",
              " 500,\n",
              " 500,\n",
              " 500,\n",
              " 500,\n",
              " 500,\n",
              " 500,\n",
              " 500,\n",
              " 500,\n",
              " 478]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_YQHpkd9cs9"
      },
      "source": [
        "And if we decode them, we can see the overlap:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "110n-0t09cs-",
        "outputId": "770cea8e-57af-4211-8b11-fa6b65f53c04"
      },
      "source": [
        "for x in tokenized_example[\"input_ids\"][:2]:\n",
        "    print(tokenizer.decode(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s>What can nuclear receptors regulate?</s></s>iNR-Drug: Predicting the Interaction of Drugs with Nuclear Receptors in Cellular Networking\n",
            "\n",
            "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3975431/\n",
            "\n",
            "SHA: ee55aea26f816403476a7cb71816b8ecb1110329\n",
            "\n",
            "Authors: Fan, Yue-Nong; Xiao, Xuan; Min, Jian-Liang; Chou, Kuo-Chen\n",
            "Date: 2014-03-19\n",
            "DOI: 10.3390/ijms15034915\n",
            "License: cc-by\n",
            "\n",
            "Abstract: Nuclear receptors (NRs) are closely associated with various major diseases such as cancer, diabetes, inflammatory disease, and osteoporosis. Therefore, NRs have become a frequent target for drug development. During the process of developing drugs against these diseases by targeting NRs, we are often facing a problem: Given a NR and chemical compound, can we identify whether they are really in interaction with each other in a cell? To address this problem, a predictor called “iNR-Drug” was developed. In the predictor, the drug compound concerned was formulated by a 256-D (dimensional) vector derived from its molecular fingerprint, and the NR by a 500-D vector formed by incorporating its sequential evolution information and physicochemical features into the general form of pseudo amino acid composition, and the prediction engine was operated by the SVM (support vector machine) algorithm. Compared with the existing prediction methods in this area, iNR-Drug not only can yield a higher success rate, but is also featured by a user-friendly web-server established at http://www.jci-bioinfo.cn/iNR-Drug/, which is particularly useful for most experimental scientists to obtain their desired data in a timely manner. It is anticipated that the iNR-Drug server may become a useful high throughput tool for both basic research and drug development, and that the current approach may be easily extended to study the interactions of drug with other targets as well.\n",
            "\n",
            "Text: With the ability to directly bind to DNA ( Figure 1 ) and regulate the expression of adjacent genes, nuclear receptors (NRs) are a class of ligand-inducible transcription factors. They regulate various biological processes, such as homeostasis, differentiation, embryonic</s>\n",
            "<s>What can nuclear receptors regulate?</s></s>/iNR-Drug/, which is particularly useful for most experimental scientists to obtain their desired data in a timely manner. It is anticipated that the iNR-Drug server may become a useful high throughput tool for both basic research and drug development, and that the current approach may be easily extended to study the interactions of drug with other targets as well.\n",
            "\n",
            "Text: With the ability to directly bind to DNA ( Figure 1 ) and regulate the expression of adjacent genes, nuclear receptors (NRs) are a class of ligand-inducible transcription factors. They regulate various biological processes, such as homeostasis, differentiation, embryonic development, and organ physiology [1] [2] [3]. The NR superfamily has been classified into seven families: NR0 (knirps or DAX like) [4, 5] ; NR1 (thyroid hormone like), NR2 (HNF4-like), NR3 (estrogen like), NR4 (nerve growth factor IB-like), NR5 (fushi tarazu-F1 like), and NR6 (germ cell nuclear factor like). Since they are involved in almost all aspects of human physiology and are implicated in many major diseases such as cancer, diabetes and osteoporosis, nuclear receptors have become major drug targets [6, 7], along with G protein-coupled receptors (GPCRs) [8] [9] [10] [11] [12] [13] [14] [15] [16] [17], ion channels [18] [19] [20], and kinase proteins [21] [22] [23] [24]. Identification of drug-target interactions is one of the most important steps for the new medicine development [25, 26]. The method usually adopted in this step is molecular docking simulation [27] [28] [29] [30] [31] [32] [33] [34] [35] [36] [37] [38] [39] [40] [41] [42] [43]. However, to make molecular docking study feasible, a reliable 3D (three dimensional) structure of the target protein is the prerequisite condition. Although X-ray crystallography is a powerful tool in determining protein 3D structures, it is time-consuming and expensive. Particularly, not all proteins</s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swGrMiXu9cs-"
      },
      "source": [
        "Now this will give us some work to properly treat the answers: we need to find in which of those features the answer actually is, and where exactly in that feature. The models we will use require the start and end positions of these answers in the tokens, so we will also need to to map parts of the original context to some tokens. Thankfully, the tokenizer we're using can help us with that by returning an `offset_mapping`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGHfmo1X9cs-",
        "outputId": "bbf72281-2ef4-4cb2-8afa-b3228393320c"
      },
      "source": [
        "tokenized_example = tokenizer(\n",
        "    example[\"question\"],\n",
        "    example[\"context\"],\n",
        "    max_length=max_length,\n",
        "    truncation=\"only_second\",\n",
        "    return_overflowing_tokens=True,\n",
        "    return_offsets_mapping=True,\n",
        "    stride=doc_stride\n",
        ")\n",
        "print(tokenized_example[\"offset_mapping\"][0][:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 0), (0, 4), (5, 8), (9, 16), (17, 26), (27, 35), (35, 36), (0, 0), (0, 0), (0, 1), (1, 3), (3, 4), (4, 8), (8, 9), (10, 17), (17, 20), (21, 24), (25, 30), (30, 36), (37, 39), (40, 45), (46, 50), (51, 58), (59, 61), (61, 65), (65, 68), (69, 71), (72, 80), (81, 88), (88, 91), (91, 92), (92, 93), (93, 98), (98, 101), (101, 104), (104, 105), (105, 107), (107, 109), (109, 110), (110, 112), (112, 113), (113, 114), (114, 117), (117, 118), (118, 121), (121, 122), (122, 124), (124, 125), (125, 126), (126, 134), (134, 135), (135, 137), (137, 138), (138, 140), (140, 143), (143, 145), (145, 146), (146, 147), (147, 148), (148, 151), (151, 152), (153, 154), (154, 155), (155, 157), (157, 160), (160, 162), (162, 163), (163, 164), (164, 166), (166, 168), (168, 170), (170, 172), (172, 173), (173, 174), (174, 176), (176, 179), (179, 181), (181, 182), (182, 183), (183, 185), (185, 186), (186, 188), (188, 191), (191, 193), (193, 194), (194, 195), (195, 199), (199, 202), (202, 203), (204, 207), (207, 208), (209, 212), (212, 213), (213, 214), (214, 217), (217, 218), (219, 223), (223, 224), (225, 229), (229, 230)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFiGXYEr9cs_"
      },
      "source": [
        "This gives, for each index of our input IDS, the corresponding start and end character in the original text that gave our token. The very first token (`[CLS]`) has (0, 0) because it doesn't correspond to any part of the question/answer, then the second token is the same as the characters 0 to 3 of the question:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jScQN1l89cs_",
        "outputId": "4b3eb1f1-a622-4f9f-e85f-5d13162a95c7"
      },
      "source": [
        "first_token_id = tokenized_example[\"input_ids\"][0][1]\n",
        "offsets = tokenized_example[\"offset_mapping\"][0][1]\n",
        "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What What\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI6r5gCo9cs_"
      },
      "source": [
        "So we can use this mapping to find the position of the start and end tokens of our answer in a given feature. We just have to distinguish which parts of the offsets correspond to the question and which part correspond to the context, this is where the `sequence_ids` method of our `tokenized_example` can be useful:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MblYjNk9cs_",
        "outputId": "eb520c85-9768-449f-935f-a27e2b799495"
      },
      "source": [
        "sequence_ids = tokenized_example.sequence_ids()\n",
        "print(sequence_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[None, 0, 0, 0, 0, 0, 0, None, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZov03gY9ctA"
      },
      "source": [
        "It returns `None` for the special tokens, then 0 or 1 depending on whether the corresponding token comes from the first sentence past (the question) or the second (the context). Now with all of this, we can find the first and last token of the answer in one of our input feature (or if the answer is not in this feature):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxHywWU-9ctA",
        "outputId": "cb3b18cf-e93c-4396-c4a9-8d9eae52e88a"
      },
      "source": [
        "answers = example[\"answers\"]\n",
        "start_char = answers[\"answer_start\"][0]\n",
        "end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "# Start token index of the current span in the text.\n",
        "token_start_index = 0\n",
        "while sequence_ids[token_start_index] != 1:\n",
        "    token_start_index += 1\n",
        "\n",
        "# End token index of the current span in the text.\n",
        "token_end_index = len(tokenized_example[\"input_ids\"][0]) - 1\n",
        "while sequence_ids[token_end_index] != 1:\n",
        "    token_end_index -= 1\n",
        "\n",
        "# Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
        "offsets = tokenized_example[\"offset_mapping\"][0]\n",
        "if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "    # Move the token_start_index and token_end_index to the two ends of the answer.\n",
        "    # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "        token_start_index += 1\n",
        "    start_position = token_start_index - 1\n",
        "    while offsets[token_end_index][1] >= end_char:\n",
        "        token_end_index -= 1\n",
        "    end_position = token_end_index + 1\n",
        "    print(start_position, end_position)\n",
        "else:\n",
        "    print(\"The answer is not in this feature.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The answer is not in this feature.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KP5Zj8X9ctA"
      },
      "source": [
        "And we can double check that it is indeed the theoretical answer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMB4MzXc9ctB"
      },
      "source": [
        "# print(tokenizer.decode(tokenized_example[\"input_ids\"][0][start_position: end_position+1]))\n",
        "# print(answers[\"text\"][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4M_X_QG9ctB"
      },
      "source": [
        "For this notebook to work with any kind of models, we need to account for the special case where the model expects padding on the left (in which case we switch the order of the question and the context):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuTolAUU9ctB"
      },
      "source": [
        "pad_on_right = tokenizer.padding_side == \"right\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNLrOvz49ctB"
      },
      "source": [
        "Now let's put everything together in one function we will apply to our training set. In the case of impossible answers (the answer is in another feature given by an example with a long context), we set the cls index for both the start and end position. We could also simply discard those examples from the training set if the flag `allow_impossible_answers` is `False`. Since the preprocessing is already complex enough as it is, we've kept is simple for this part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0QaapZj9ctC"
      },
      "source": [
        "def prepare_train_features(examples):\n",
        "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
        "    # in one example possible giving several features when a context is long, each of those features having a\n",
        "    # context that overlaps a bit the context of the previous feature.\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\" if pad_on_right else \"context\"],\n",
        "        examples[\"context\" if pad_on_right else \"question\"],\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "    # its corresponding example. This key gives us just that.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
        "    # help us compute the start_positions and end_positions.\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "    # Let's label those examples!\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        # We will label impossible answers with the index of the CLS token.\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = examples[\"answers\"][sample_index]\n",
        "        # If no answers are given, set the cls_index as answer.\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Start/end character index of the answer in the text.\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            # Start token index of the current span in the text.\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
        "                token_start_index += 1\n",
        "\n",
        "            # End token index of the current span in the text.\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
        "                token_end_index -= 1\n",
        "\n",
        "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "            else:\n",
        "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "    return tokenized_examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lm8ozrJIrJR"
      },
      "source": [
        "This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-b70jh26IrJS"
      },
      "source": [
        "features = prepare_train_features(datasets['train'][:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS-6iXTkIrJT"
      },
      "source": [
        "To apply this function on all the sentences (or pairs of sentences) in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command. Since our preprocessing changes the number of samples, we need to remove the old columns when applying it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "e181d12d7990447f9247fb42c5690359",
            "98fa55a47b0742d3925f6ee99fd1a1dd",
            "fea4c112ef464e7f84eda0429b190e36",
            "c9f938f970ea4736a3e6d7d00d4c12ef",
            "ff9e01352f644059b716c0ecab7bcac5",
            "c5e392adda61409a992f2bb26c6f9703",
            "b9c55dc3f966404ea2135b91c9c1bb99",
            "44cfb14a0323448aa773cfeb006be369",
            "648c7cba7b9246c2adc76dfacc56f035",
            "f5707967437d4073a400cf3264573bf6",
            "e05e419c88ee4364978d9ca52b1e6801",
            "5b18a06e2b294c2094f80a2d00e2270d",
            "c7a26ef462c64596beee2fbe8d1f2dab",
            "590714fc719b42c1a0d214cffce7299c",
            "782c161bada94987ade01845b6377597",
            "92163e0ed63148f78f5bd6dc333aa479"
          ]
        },
        "id": "DDtsaJeVIrJT",
        "outputId": "9de5cb1f-ef20-4036-8810-ca0e72892592"
      },
      "source": [
        "tokenized_datasets = datasets.map(prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e181d12d7990447f9247fb42c5690359",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "648c7cba7b9246c2adc76dfacc56f035",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voWiw8C7IrJV"
      },
      "source": [
        "Even better, the results are automatically cached by the 🤗 Datasets library to avoid spending time on this step the next time you run your notebook. The 🤗 Datasets library is normally smart enough to detect when the function you pass to map has changed (and thus requires to not use the cache data). For instance, it will properly detect if you change the task in the first cell and rerun the notebook. 🤗 Datasets warns you when it uses cached files, you can pass `load_from_cache_file=False` in the call to `map` to not use the cached files and force the preprocessing to be applied again.\n",
        "\n",
        "Note that we passed `batched=True` to encode the texts by batches together. This is to leverage the full benefit of the fast tokenizer we loaded earlier, which will use multi-threading to treat the texts in a batch concurrently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "## Fine-tuning the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBiW8UpKIrJW"
      },
      "source": [
        "Now that our data is ready for training, we can download the pretrained model and fine-tune it. Since our task is question answering, we use the `AutoModelForQuestionAnswering` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "b671a9cd6cbe48c2bc8cca26f03b61a4",
            "46723feb4f2f43aea7583f0b075e5e4b",
            "d3dad21ac94c4ef7a569dde62221fe0b",
            "9f841c51ba0947b298d2a897e502b36e",
            "6c91a79738e9416a8bac9c370fe71343",
            "5e792c123efd42be8388ecf72e484c8f",
            "99c538b16a444d25a329b2ad03f9062e",
            "78eb26f05c124087ba4f989fe63680f1"
          ]
        },
        "id": "TlqNaB8jIrJW",
        "outputId": "fbc8cf00-e455-413f-9293-df1e85263319"
      },
      "source": [
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b671a9cd6cbe48c2bc8cca26f03b61a4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=498637768.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CczA5lJlIrJX"
      },
      "source": [
        "The warning is telling us we are throwing away some weights (the `vocab_transform` and `vocab_layer_norm` layers) and randomly initializing some other (the `pre_classifier` and `classifier` layers). This is absolutely normal in this case, because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N8urzhyIrJY"
      },
      "source": [
        "To instantiate a `Trainer`, we will need to define three more things. The most important is the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bliy8zgjIrJY"
      },
      "source": [
        "args = TrainingArguments(\n",
        "    f\"test-squad\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km3pGVdTIrJc"
      },
      "source": [
        "Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the `batch_size` defined at the top of the notebook and customize the number of epochs for training, as well as the weight decay."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHpKDZM-9ctF"
      },
      "source": [
        "Then we will need a data collator that will batch our processed examples together, here the default one will work:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILmIwJaA9ctF"
      },
      "source": [
        "from transformers import default_data_collator\n",
        "\n",
        "data_collator = default_data_collator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXuFTAzDIrJe"
      },
      "source": [
        "We will evaluate our model and compute metrics in the next section (this is a very long operation, so we will only compute the evaluation loss during training).\n",
        "\n",
        "Then we just need to pass all of this along with our datasets to the `Trainer`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imY1oC3SIrJf"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdzABDVcIrJg"
      },
      "source": [
        "We can now finetune our model by just calling the `train` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNx5pyRlIrJh"
      },
      "source": [
        "# trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9f1Z0rb9ctG"
      },
      "source": [
        "Since this training is particularly long, let's save the model just in case we need to restart."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rLQGSnD9ctG"
      },
      "source": [
        "# trainer.save_model(\"distil-covid-trained\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nPvAZtk9ctG"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPN7XhHC9ctH"
      },
      "source": [
        "Evaluating our model will require a bit more work, as we will need to map the predictions of our model back to parts of the context. The model itself predicts logits for the start and en position of our answers: if we take a batch from our validation datalaoder, here is the output our model gives us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYkp5oK29ctH",
        "outputId": "4c429d90-016f-4c56-dc98-3eca2109b941"
      },
      "source": [
        "import torch\n",
        "\n",
        "for batch in trainer.get_eval_dataloader():\n",
        "    break\n",
        "batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\n",
        "with torch.no_grad():\n",
        "    output = trainer.model(**batch)\n",
        "output.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys(['loss', 'start_logits', 'end_logits'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtpoYs0z9ctI"
      },
      "source": [
        "The output of the model is a dict-like object that contains the loss (since we provided labels), the start and end logits. We won't need the loss for our predictions, let's have a look a the logits:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oX6ScXqh9ctI",
        "outputId": "c770dce1-1a07-43ea-dba6-2c764efa5620"
      },
      "source": [
        "output.start_logits.shape, output.end_logits.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([16, 500]), torch.Size([16, 500]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAU7jFAM9ctI"
      },
      "source": [
        "We have one logit for each feature and each token. The most obvious thing to predict an answer for each featyre is to take the index for the maximum of the start logits as a start position and the index of the maximum of the end logits as an end position."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moqvOMsp9ctI",
        "outputId": "0420d406-7b4c-4695-c44b-ee91fc2e8a04"
      },
      "source": [
        "output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([153,   0,   0, 413,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0], device='cuda:0'),\n",
              " tensor([175,  78,  71,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0], device='cuda:0'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJMg6A6t9ctI"
      },
      "source": [
        "This will work great in a lot of cases, but what if this prediction gives us something impossible: the start position could be greater than the end position, or point to a span of text in the question instead of the answer. In that case, we might want to look at the second best prediction to see if it gives a possible answer and select that instead.\n",
        "\n",
        "However, picking the second best answer is not as easy as picking the best one: is it the second best index in the start logits with the best index in the end logits? Or the best index in the start logits with the second best index in the end logits? And if that second best answer is not possible either, it gets even trickier for the third best answer.\n",
        "\n",
        "\n",
        "To classify our answers, we will use the score obtained by adding the start and end logits. We won't try to order all the possible answers and limit ourselves to with a hyper-parameter we call `n_best_size`. We'll pick the best indices in the start and end logits and gather all the answers this predicts. After checking if each one is valid, we will sort them by their score and keep the best one. Here is how we would do this on the first feature in the batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amv6pJ_B9ctJ"
      },
      "source": [
        "n_best_size = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICFbFU-19ctJ"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "start_logits = output.start_logits[0].cpu().numpy()\n",
        "end_logits = output.end_logits[0].cpu().numpy()\n",
        "# Gather the indices the best start/end logits:\n",
        "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "valid_answers = []\n",
        "for start_index in start_indexes:\n",
        "    for end_index in end_indexes:\n",
        "        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n",
        "            valid_answers.append(\n",
        "                {\n",
        "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                    \"text\": \"\" # We need to find a way to get back the original substring corresponding to the answer in the context\n",
        "                }\n",
        "            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFjwP8sH9ctJ"
      },
      "source": [
        "And then we can sort the `valid_answers` according to their `score` and only keep the best one. The only point left is how to check a given span is inside the context (and not the question) and how to get back the text inside. To do this, we need to add two things to our validation features:\n",
        "- the ID of the example that generated the feature (since each example can generate several features, as seen before);\n",
        "- the offset mapping that will give us a map from token indices to character positions in the context.\n",
        "\n",
        "That's why we will re-process the validation set with the following function, slightly different from `prepare_train_features`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OL83DAkA9ctJ"
      },
      "source": [
        "def prepare_validation_features(examples):\n",
        "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
        "    # in one example possible giving several features when a context is long, each of those features having a\n",
        "    # context that overlaps a bit the context of the previous feature.\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\" if pad_on_right else \"context\"],\n",
        "        examples[\"context\" if pad_on_right else \"question\"],\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "    # its corresponding example. This key gives us just that.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    # We keep the example_id that gave us this feature and we will store the offset mappings.\n",
        "    tokenized_examples[\"example_id\"] = []\n",
        "\n",
        "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "        context_index = 1 if pad_on_right else 0\n",
        "\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
        "\n",
        "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
        "        # position is part of the context or not.\n",
        "        tokenized_examples[\"offset_mapping\"][i] = [\n",
        "            (o if sequence_ids[k] == context_index else None)\n",
        "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
        "        ]\n",
        "\n",
        "    return tokenized_examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35-KjpxL9ctJ"
      },
      "source": [
        "And like before, we can apply that function to our validation set easily:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "1665f22fa1754e169082e4cdfd73fc09",
            "931b8585ad2748fb8ade27e0e26c2d67",
            "de90c21651f5453b862739456d81b96d",
            "6f0c56daa66c43c998fc85ef671299d5",
            "e372e6baab444b2d8a5d0b1acebe6daa",
            "dd4656abe3514bc2b4d932c06ed741c7",
            "ec1981c4de6e4fdbb2f39997ce30b5b5",
            "6b6c495342d347dc8b4cc23666623eb7"
          ]
        },
        "id": "rr5VEPrW9ctK",
        "outputId": "f5553e74-3a27-468c-81df-6e84221ae082"
      },
      "source": [
        "validation_features = datasets[\"validation\"].map(\n",
        "    prepare_validation_features,\n",
        "    batched=True,\n",
        "    remove_columns=datasets[\"validation\"].column_names\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1665f22fa1754e169082e4cdfd73fc09",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amE-12DP9ctL"
      },
      "source": [
        "Now we can grab the predictions for all features by using the `Trainer.predict` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "x43ECvME9ctL",
        "outputId": "e2899b88-4e89-4b97-ef39-f60aa0133f0b"
      },
      "source": [
        "raw_predictions = trainer.predict(validation_features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='260' max='260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [260/260 02:13]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CPRYpDz9ctM"
      },
      "source": [
        "The `Trainer` *hides* the columns that are not used by the model (here `example_id` and `offset_mapping` which we will need for our post-processing), so we set them back:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EP2mqlQ29ctM"
      },
      "source": [
        "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tvgv38H9ctM"
      },
      "source": [
        "We can now refine the test we had before: since we set `None` in the offset mappings when it corresponds to a part of the question, it's easy to check if an answer is fully inside the context. We also eliminate very long answers from our considerations (with an hyper-parameter we can tune)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67KqQEoe9ctM"
      },
      "source": [
        "max_answer_length = 60"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLBwOxk39ctN",
        "outputId": "b0b7b60d-0a71-4a4d-da30-bf9fa8ce67b8"
      },
      "source": [
        "start_logits = output.start_logits[0].cpu().numpy()\n",
        "end_logits = output.end_logits[0].cpu().numpy()\n",
        "offset_mapping = validation_features[0][\"offset_mapping\"]\n",
        "# The first feature comes from the first example. For the more general case, we will need to be match the example_id to\n",
        "# an example index\n",
        "context = datasets[\"validation\"][0][\"context\"]\n",
        "\n",
        "# Gather the indices the best start/end logits:\n",
        "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "valid_answers = []\n",
        "for start_index in start_indexes:\n",
        "    for end_index in end_indexes:\n",
        "        # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
        "        # to part of the input_ids that are not in the context.\n",
        "        if (\n",
        "            start_index >= len(offset_mapping)\n",
        "            or end_index >= len(offset_mapping)\n",
        "            or offset_mapping[start_index] is None\n",
        "            or offset_mapping[end_index] is None\n",
        "        ):\n",
        "            continue\n",
        "        # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
        "        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "            continue\n",
        "        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n",
        "            start_char = offset_mapping[start_index][0]\n",
        "            end_char = offset_mapping[end_index][1]\n",
        "            valid_answers.append(\n",
        "                {\n",
        "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                    \"text\": context[start_char: end_char]\n",
        "                }\n",
        "            )\n",
        "\n",
        "valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
        "valid_answers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 11.464323,\n",
              "  'text': 'Mother-to-child transmission (MTCT) is the main cause of HIV-1 infection in children worldwide.'},\n",
              " {'score': 9.37315,\n",
              "  'text': 'Mother-to-child transmission (MTCT) is the main cause of HIV-1 infection in children worldwide'},\n",
              " {'score': 7.5647163, 'text': 'worldwide.'},\n",
              " {'score': 6.1717076, 'text': 'Mother-to-child transmission (MTCT)'},\n",
              " {'score': 5.685243, 'text': 'Mother-to-child transmission'},\n",
              " {'score': 5.4735436, 'text': 'worldwide'},\n",
              " {'score': 5.3105655,\n",
              "  'text': ': Mother-to-child transmission (MTCT) is the main cause of HIV-1 infection in children worldwide.'},\n",
              " {'score': 4.5358906,\n",
              "  'text': 'transmission (MTCT) is the main cause of HIV-1 infection in children worldwide.'},\n",
              " {'score': 4.413592, 'text': '.'},\n",
              " {'score': 4.3732033,\n",
              "  'text': 'MTCT) is the main cause of HIV-1 infection in children worldwide.'},\n",
              " {'score': 3.938302, 'text': 'Mother-to-child transmission (MTCT'},\n",
              " {'score': 3.5990171,\n",
              "  'text': ') is the main cause of HIV-1 infection in children worldwide.'},\n",
              " {'score': 3.2193923,\n",
              "  'text': ': Mother-to-child transmission (MTCT) is the main cause of HIV-1 infection in children worldwide'},\n",
              " {'score': 2.9015524, 'text': 'children worldwide.'},\n",
              " {'score': 2.557309,\n",
              "  'text': 'the main cause of HIV-1 infection in children worldwide.'},\n",
              " {'score': 2.4447174,\n",
              "  'text': 'transmission (MTCT) is the main cause of HIV-1 infection in children worldwide'},\n",
              " {'score': 2.4312599,\n",
              "  'text': '(MTCT) is the main cause of HIV-1 infection in children worldwide.'},\n",
              " {'score': 2.34821, 'text': 'Mother'},\n",
              " {'score': 2.2820303,\n",
              "  'text': 'MTCT) is the main cause of HIV-1 infection in children worldwide'},\n",
              " {'score': 2.1433566,\n",
              "  'text': 'main cause of HIV-1 infection in children worldwide.'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_X6jPvS9ctN"
      },
      "source": [
        "We can compare to the actual ground-truth answer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpehUdr29ctN",
        "outputId": "7fae23d7-cfef-43fc-d8e4-2c4d975710af"
      },
      "source": [
        "datasets[\"validation\"][0][\"answers\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer_start': [370],\n",
              " 'text': ['Mother-to-child transmission (MTCT) is the main cause of HIV-1 infection in children worldwide.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uXbNGJR9ctN"
      },
      "source": [
        "Our model picked the right as the most likely answer!\n",
        "\n",
        "As we mentioned in the code above, this was easy on the first feature because we knew it comes from the first example. For the other features, we will need a map between examples and their corresponding features. Also, since one example can give several features, we will need to gather together all the answers in all the features generated by a given example, then pick the best one. The following code builds a map from example index to its corresponding features indices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS7E-65_9ctO"
      },
      "source": [
        "import collections\n",
        "\n",
        "examples = datasets[\"validation\"]\n",
        "features = validation_features\n",
        "\n",
        "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "features_per_example = collections.defaultdict(list)\n",
        "for i, feature in enumerate(features):\n",
        "    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROM9a3Ex9ctO"
      },
      "source": [
        "We're almost ready for our post-processing function. The last bit to deal with is the impossible answer (when `squad_v2 = True`). The code above only keeps answers that are inside the context, we need to also grab the score for the impossible answer (which has start and end indices corresponding to the index of the CLS token). When one example gives several features, we have to predict the impossible answer when all the features give a high score to the impossible answer (since one feature could predict the impossible answer just because the answer isn't in the part of the context it has access too), which is why the score of the impossible answer for one example is the *minimum* of the scores for the impossible answer in each feature generated by the example.\n",
        "\n",
        "We then predict the impossible answer when that score is greater than the score of the best non-impossible answer. All combined together, this gives us this post-processing function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-hMtfYr9ctO"
      },
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
        "    all_start_logits, all_end_logits = raw_predictions\n",
        "    # Build a map example to its corresponding features.\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = collections.defaultdict(list)\n",
        "    for i, feature in enumerate(features):\n",
        "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
        "\n",
        "    # The dictionaries we have to fill.\n",
        "    predictions = collections.OrderedDict()\n",
        "\n",
        "    # Logging.\n",
        "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
        "\n",
        "    # Let's loop over all the examples!\n",
        "    for example_index, example in enumerate(tqdm(examples)):\n",
        "        # Those are the indices of the features associated to the current example.\n",
        "        feature_indices = features_per_example[example_index]\n",
        "\n",
        "        min_null_score = None # Only used if squad_v2 is True.\n",
        "        valid_answers = []\n",
        "        \n",
        "        context = example[\"context\"]\n",
        "        # Looping through all the features associated to the current example.\n",
        "        for feature_index in feature_indices:\n",
        "            # We grab the predictions of the model for this feature.\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
        "            # context.\n",
        "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
        "\n",
        "            # Update minimum null prediction.\n",
        "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
        "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
        "            if min_null_score is None or min_null_score < feature_null_score:\n",
        "                min_null_score = feature_null_score\n",
        "\n",
        "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
        "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
        "                    # to part of the input_ids that are not in the context.\n",
        "                    if (\n",
        "                        start_index >= len(offset_mapping)\n",
        "                        or end_index >= len(offset_mapping)\n",
        "                        or offset_mapping[start_index] is None\n",
        "                        or offset_mapping[end_index] is None\n",
        "                    ):\n",
        "                        continue\n",
        "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
        "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "\n",
        "                    start_char = offset_mapping[start_index][0]\n",
        "                    end_char = offset_mapping[end_index][1]\n",
        "                    valid_answers.append(\n",
        "                        {\n",
        "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                            \"text\": context[start_char: end_char]\n",
        "                        }\n",
        "                    )\n",
        "        \n",
        "        if len(valid_answers) > 0:\n",
        "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
        "        else:\n",
        "            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
        "            # failure.\n",
        "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
        "        \n",
        "        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n",
        "        if not squad_v2:\n",
        "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
        "        else:\n",
        "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
        "            predictions[example[\"id\"]] = answer\n",
        "\n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYvajPWv9ctO"
      },
      "source": [
        "And we can apply our post-processing function to our raw predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPxBukOn9ctO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "e97bf3ca5e084f70b58ffe6713ec50be",
            "459dba16a7dc4e28a383aa3b89f97092",
            "ae4399e23f734edf8ffa9ceb32c888f8",
            "87b5d966b952402ba788e8cf678c62c5",
            "fd4c8cd51d83460f8c7dfd43a78db358",
            "e281bc087217403ebb5fc2d2d9f62eca",
            "e7512d90ba7b4795bd974405254c48ec",
            "9a426f65cc3c4b23887b165a75bf4d38"
          ]
        },
        "outputId": "426165d4-dd5d-4866-9749-f56eef243e5f"
      },
      "source": [
        "final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Post-processing 300 example predictions split into 4149 features.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e97bf3ca5e084f70b58ffe6713ec50be",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=300.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9wG7Z0Am5I3"
      },
      "source": [
        "# import pickle\n",
        "import json\n",
        "\n",
        "# with open('output-distil-covid.pickle', 'wb') as handle:\n",
        "#     pickle.dump(final_predictions, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "with open('roberta-covid.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(final_predictions, f, indent=4, ensure_ascii=False)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st7yL4cAneyn"
      },
      "source": [
        "# import pickle\n",
        "# file = open(\"output-distil-covid.pickle\",'rb')\n",
        "# object_file = pickle.load(file)\n",
        "# file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7Kd5usCPiBp"
      },
      "source": [
        "# import pickle\n",
        "\n",
        "\n",
        "# with open('output-distil-covid.pickle', 'wb') as handle:\n",
        "#     pickle.dump(final_predictions, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# with open('{}-covid-1.json'.format(\"distil\"), 'w', encoding='utf-8') as f:\n",
        "#     json.dump(object_file, f, indent=4, ensure_ascii=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ln2JjDYzFv4S"
      },
      "source": [
        "from __future__ import print_function\n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "import argparse\n",
        "import json\n",
        "import sys\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klQXV54yNVhX"
      },
      "source": [
        "def f1_score(prediction, ground_truth):\n",
        "    prediction_tokens = prediction.split()\n",
        "    ground_truth_tokens = ground_truth.split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return (prediction == ground_truth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHiyOSR7NZ84"
      },
      "source": [
        "my_predictions = list(final_predictions.items())\n",
        "\n",
        "for i in range(0,len(my_predictions)):\n",
        "  my_predictions[i] = normalize_answer(my_predictions[i][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnGuX984OK_X"
      },
      "source": [
        "orig = list(datasets[\"validation\"])\n",
        "for i in range(0,len(orig)):\n",
        "  orig[i] = normalize_answer(orig[i][\"answers\"][\"text\"][0])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLXyEGf-DWL9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b0d4b5b-08f0-42dc-f037-88ff939ece5c"
      },
      "source": [
        "em = 0\n",
        "for i in range(0,len(orig)):\n",
        "  if orig[i] == my_predictions[i]: em+=1\n",
        "\n",
        "print(\"Exact Match {}\".format(em/len(orig)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exact Match 0.4666666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF5eIOfUMCnM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aba0c1ae-4cfe-4116-f7c0-a107106c6168"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1 = f1_score(orig, my_predictions, average='macro')\n",
        "print(\"f1_score {} macro\".format(f1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1_score 0.30866965620328846 macro\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e0eTJudIEQy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "245795df-a1f6-458d-a6c9-3b2f1d3ca659"
      },
      "source": [
        "orig[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'dcsignr plays crucial role in mtct of hiv1 and that impaired placental dcsignr expression increases risk of transmission'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCE5NyBL2G4e",
        "outputId": "bd82a687-6bb2-4173-e317-eae68deebab0"
      },
      "source": [
        "f1 = f1_score(orig[1].split(), my_predictions[1].split(), average='macro')\n",
        "f1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD67HWs62vm1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}